{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec1c145e-90fc-4156-bd11-ad8c974af193",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "799c3c35-feba-433b-bfd8-3aff5491b1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    The sigmoid function.\n",
    "\n",
    "    Parameters:\n",
    "    z (numpy.ndarray): The input to the sigmoid function.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: The sigmoid of the input.\n",
    "    \"\"\"\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"\n",
    "    Derivative of the sigmoid function.\n",
    "\n",
    "    Parameters:\n",
    "    z (numpy.ndarray): The input to the sigmoid function.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: The derivative of the sigmoid function at z.\n",
    "    \"\"\"\n",
    "    return sigmoid(z) * (1 - sigmoid(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0589bef1-dfd4-4bd9-8629-0e27b790685f",
   "metadata": {},
   "source": [
    "## 3-layer NN\n",
    "![This is an example image](http://neuralnetworksanddeeplearning.com/images/tikz10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe57e65f-0036-4663-81fb-c2ee4f15ffa5",
   "metadata": {},
   "source": [
    "## Weights initilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ca779ec-ef79-4976-a20d-194906f223e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 4\n",
      "4 1\n",
      "[array([[ 0.45755238, -0.63902368, -1.42559792],\n",
      "       [ 1.02520757, -0.22282446, -0.37816523],\n",
      "       [ 0.17191112,  2.60076811, -0.61025091],\n",
      "       [ 0.40555358, -1.18517198, -2.41139182]]), array([[ 1.03100247,  1.98062682, -1.7508068 , -0.23435663]])]\n"
     ]
    }
   ],
   "source": [
    "# Size contains the number of neurons in the respective layers\n",
    "sizes = [3, 4, 1]\n",
    "# Understand dimensions of weights\n",
    "for x, y in zip(sizes[:-1], sizes[1:]):\n",
    "    print(x, y)\n",
    "# Initialize weights using Gaussian distribution with mean 0 and standard deviation 1\n",
    "weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd0643a-df10-4fd8-8d9f-3669c4f40ad9",
   "metadata": {},
   "source": [
    "For the above network, the number of weights for each neuron in a layer is determined by the number of neurons in the previous layer. For instance, the second layer (i.e., hidden layer) in the above 3-layer NN has four neurons. For each neuron in this layer there are three inputs from the three neurons in the previous layer, which requires three weights. Therefore, the dimension of the array to store all the weights for this layer has shape of (4, 3), with the first dimension corresponds to the number of neurons in the current layer and second dimension corresponds to the total number of neurons in the previous layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d74278-2934-4be9-a0b0-09a287b61515",
   "metadata": {},
   "source": [
    "## Bias initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75829ff4-c53a-4ea2-9ef5-481825cd0799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "1\n",
      "[array([[0.98872931],\n",
      "       [0.57179665],\n",
      "       [0.27104466],\n",
      "       [1.12685389]]), array([[0.11113971]])]\n"
     ]
    }
   ],
   "source": [
    "# Understand dimensions of bias\n",
    "for y in sizes[1:]:\n",
    "    print(y)\n",
    "# Initialize bias using Gaussian distribution with mean 0 and standard deviation 1    \n",
    "bias = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "print(bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d04318-e32f-448b-bd18-f24197f23b2b",
   "metadata": {},
   "source": [
    "For bias, firstly no bias term is needed for the input layer since the neurons in this layer do not have any outputs coming in. The number of bias terms for all the subsequent layes, including output layer, is equal to the number of neurons in each layer, which is captured in the sizes of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0878c023-70f7-409f-a2e6-772c259018c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self, sizes):\n",
    "        \"\"\"\n",
    "        Initialize the neural network with the given sizes.\n",
    "        \n",
    "        Parameters:\n",
    "        sizes (list): A list containing the number of neurons in each layer.\n",
    "                      For example, [784, 30, 10] would create a network with\n",
    "                      784 input neurons, one hidden layer with 30 neurons,\n",
    "                      and an output layer with 10 neurons.\n",
    "        \"\"\"\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        # Biases and weights are initialized with Gaussian distribution\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "\n",
    "    def feedforward(self, a):\n",
    "        \"\"\"\n",
    "        Return the output of the network if 'a' is input.\n",
    "        \n",
    "        Parameters:\n",
    "        a (numpy array): The input to the network.\n",
    "        \n",
    "        Returns:\n",
    "        numpy array: The output of the network.\n",
    "        \"\"\"\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a) + b)\n",
    "        return a\n",
    "\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n",
    "        \"\"\"\n",
    "        Train the neural network using mini-batch stochastic gradient descent.\n",
    "        \n",
    "        Parameters:\n",
    "        training_data (list): A list of tuples '(x, y)' representing the training inputs\n",
    "                              and the desired outputs.\n",
    "        epochs (int): The number of epochs to train for.\n",
    "        mini_batch_size (int): The size of the mini-batches to use when sampling.\n",
    "        eta (float): The learning rate.\n",
    "        test_data (list, optional): If provided, the network will be evaluated\n",
    "                                    against the test data after each epoch, and\n",
    "                                    partial progress will be printed out.\n",
    "        \"\"\"\n",
    "        if test_data: n_test = len(test_data)\n",
    "        n = len(training_data)\n",
    "        for j in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [training_data[k:k+mini_batch_size] for k in range(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            if test_data:\n",
    "                print(f\"Epoch {j}: {self.evaluate(test_data)} / {n_test}\")\n",
    "            else:\n",
    "                print(f\"Epoch {j} complete\")\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        \"\"\"\n",
    "        Update the network's weights and biases by applying gradient descent\n",
    "        using backpropagation to a single mini-batch.\n",
    "        \n",
    "        Parameters:\n",
    "        mini_batch (list): A list of tuples '(x, y)'.\n",
    "        eta (float): The learning rate.\n",
    "        \"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb + dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw + dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [w - (eta / len(mini_batch)) * nw for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b - (eta / len(mini_batch)) * nb for b, nb in zip(self.biases, nabla_b)]\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"\n",
    "        Return a tuple representing the gradient for the cost function.\n",
    "        \n",
    "        Parameters:\n",
    "        x (numpy array): The input to the network.\n",
    "        y (numpy array): The desired output.\n",
    "        \n",
    "        Returns:\n",
    "        tuple: (nabla_b, nabla_w), representing the gradients for the biases and weights.\n",
    "        \"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # Feedforward\n",
    "        activation = x\n",
    "        activations = [x]  # List to store all the activations, layer by layer\n",
    "        zs = []  # List to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation) + b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        # Backward pass\n",
    "        delta = self.cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        # Update the gradients for the previous layers\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "    def evaluate(self, test_data):\n",
    "        \"\"\"\n",
    "        Return the number of test inputs for which the neural network outputs\n",
    "        the correct result.\n",
    "        \n",
    "        Parameters:\n",
    "        test_data (list): A list of tuples '(x, y)' where 'x' is the input and 'y' is the desired output.\n",
    "        \n",
    "        Returns:\n",
    "        int: The number of test inputs for which the network is correct.\n",
    "        \"\"\"\n",
    "        test_results = [(np.argmax(self.feedforward(x)), y) for (x, y) in test_data]\n",
    "        return sum(int(x == y) for (x, y) in test_results)\n",
    "\n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        \"\"\"\n",
    "        Return the vector of partial derivatives ∂C/∂a for the output activations.\n",
    "        \n",
    "        Parameters:\n",
    "        output_activations (numpy array): The output of the network.\n",
    "        y (numpy array): The desired output.\n",
    "        \n",
    "        Returns:\n",
    "        numpy array: The derivative of the cost function.\n",
    "        \"\"\"\n",
    "        return (output_activations - y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d4af14-972b-441c-b20c-cf6ad5313462",
   "metadata": {},
   "source": [
    "## Create a Network object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ea457f3-eac3-4931-9196-92ae79db4fda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A neural network with three layers: one input layer with three neurons, one hidden layer with four neurons and \n",
    "# one output layer with one neuron\n",
    "net = Network([3, 4, 1])\n",
    "# Weights connecting the second layer and third layers of neurons\n",
    "net.weights[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28dbc1d-5af7-44be-945e-e6b8627c216c",
   "metadata": {},
   "source": [
    "## Understand `feedforward` function of `Network` object\n",
    "Let's walk through an example of the `feedforward` function using a neural network with sizes `[3, 4, 1]`. This means the network has:\n",
    "- 3 neurons in the input layer\n",
    "- 4 neurons in the hidden layer\n",
    "- 1 neuron in the output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacc50d2-6b44-4a8c-ac31-cab5715e764f",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "For the sake of this example, let's assume the following initial random weights and biases for the network:\n",
    "\n",
    "#### Weights\n",
    "- Between input layer and hidden layer (3 input neurons to 4 hidden neurons):\n",
    "  ```python\n",
    "  w1 = [[0.2, -0.5, 1.0],\n",
    "        [1.5, -1.0, 0.5],\n",
    "        [-1.5, 2.0, -1.0],\n",
    "        [0.5, -0.5, 1.5]]\n",
    "  ```\n",
    "- Between hidden layer and output layer (4 hidden neurons to 1 output neuron):\n",
    "  ```python\n",
    "  w2 = [[0.3, -0.8, 0.5, 1.0]]\n",
    "  ```\n",
    "\n",
    "#### Biases\n",
    "- For hidden layer (4 neurons):\n",
    "  ```python\n",
    "  b1 = [[0.1],\n",
    "        [0.2],\n",
    "        [0.3],\n",
    "        [0.4]]\n",
    "  ```\n",
    "- For output layer (1 neuron):\n",
    "  ```python\n",
    "  b2 = [[0.5]]\n",
    "  ```\n",
    "\n",
    "### Input\n",
    "Input layer of this NN has three neurons. Let's take a specific input training example for those three input neurons:\n",
    "```python\n",
    "input_vector = [[0.5],\n",
    "                [0.1],\n",
    "                [0.4]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ed5b89e-0d38-4579-8abf-881f1628733a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<svg width=\"800\" height=\"400\" viewBox=\"0 0 800 400\" xmlns=\"http://www.w3.org/2000/svg\">\n",
       "  <svg viewBox=\"0 0 800 400\" xmlns=\"http://www.w3.org/2000/svg\">\n",
       "  <!-- Input Layer -->\n",
       "  <g id=\"input-layer\">\n",
       "    <circle cx=\"100\" cy=\"80\" r=\"20\" fill=\"lightblue\" stroke=\"black\"/>\n",
       "    <text x=\"100\" y=\"85\" text-anchor=\"middle\" font-size=\"12\">0.5</text>\n",
       "    <circle cx=\"100\" cy=\"160\" r=\"20\" fill=\"lightblue\" stroke=\"black\"/>\n",
       "    <text x=\"100\" y=\"165\" text-anchor=\"middle\" font-size=\"12\">0.1</text>\n",
       "    <circle cx=\"100\" cy=\"240\" r=\"20\" fill=\"lightblue\" stroke=\"black\"/>\n",
       "    <text x=\"100\" y=\"245\" text-anchor=\"middle\" font-size=\"12\">0.4</text>\n",
       "  </g>\n",
       "  \n",
       "  <!-- Hidden Layer -->\n",
       "  <g id=\"hidden-layer\">\n",
       "    <circle cx=\"300\" cy=\"60\" r=\"20\" fill=\"lightgreen\" stroke=\"black\"/>\n",
       "    <text x=\"300\" y=\"65\" text-anchor=\"middle\" font-size=\"12\">0.63</text>\n",
       "    <circle cx=\"300\" cy=\"140\" r=\"20\" fill=\"lightgreen\" stroke=\"black\"/>\n",
       "    <text x=\"300\" y=\"145\" text-anchor=\"middle\" font-size=\"12\">0.74</text>\n",
       "    <circle cx=\"300\" cy=\"220\" r=\"20\" fill=\"lightgreen\" stroke=\"black\"/>\n",
       "    <text x=\"300\" y=\"225\" text-anchor=\"middle\" font-size=\"12\">0.34</text>\n",
       "    <circle cx=\"300\" cy=\"300\" r=\"20\" fill=\"lightgreen\" stroke=\"black\"/>\n",
       "    <text x=\"300\" y=\"305\" text-anchor=\"middle\" font-size=\"12\">0.77</text>\n",
       "  </g>\n",
       "  \n",
       "  <!-- Output Layer -->\n",
       "  <g id=\"output-layer\">\n",
       "    <circle cx=\"500\" cy=\"160\" r=\"20\" fill=\"lightyellow\" stroke=\"black\"/>\n",
       "    <text x=\"500\" y=\"165\" text-anchor=\"middle\" font-size=\"12\">0.74</text>\n",
       "  </g>\n",
       "  \n",
       "  <!-- Connections -->\n",
       "  <g id=\"connections\" stroke=\"gray\" stroke-width=\"1\">\n",
       "    <!-- Input to Hidden -->\n",
       "    <line x1=\"120\" y1=\"80\" x2=\"280\" y2=\"60\"/>\n",
       "    <line x1=\"120\" y1=\"80\" x2=\"280\" y2=\"140\"/>\n",
       "    <line x1=\"120\" y1=\"80\" x2=\"280\" y2=\"220\"/>\n",
       "    <line x1=\"120\" y1=\"80\" x2=\"280\" y2=\"300\"/>\n",
       "    \n",
       "    <line x1=\"120\" y1=\"160\" x2=\"280\" y2=\"60\"/>\n",
       "    <line x1=\"120\" y1=\"160\" x2=\"280\" y2=\"140\"/>\n",
       "    <line x1=\"120\" y1=\"160\" x2=\"280\" y2=\"220\"/>\n",
       "    <line x1=\"120\" y1=\"160\" x2=\"280\" y2=\"300\"/>\n",
       "    \n",
       "    <line x1=\"120\" y1=\"240\" x2=\"280\" y2=\"60\"/>\n",
       "    <line x1=\"120\" y1=\"240\" x2=\"280\" y2=\"140\"/>\n",
       "    <line x1=\"120\" y1=\"240\" x2=\"280\" y2=\"220\"/>\n",
       "    <line x1=\"120\" y1=\"240\" x2=\"280\" y2=\"300\"/>\n",
       "    \n",
       "    <!-- Hidden to Output -->\n",
       "    <line x1=\"320\" y1=\"60\" x2=\"480\" y2=\"160\"/>\n",
       "    <line x1=\"320\" y1=\"140\" x2=\"480\" y2=\"160\"/>\n",
       "    <line x1=\"320\" y1=\"220\" x2=\"480\" y2=\"160\"/>\n",
       "    <line x1=\"320\" y1=\"300\" x2=\"480\" y2=\"160\"/>\n",
       "  </g>\n",
       "  \n",
       "  <!-- Weights -->\n",
       "  <g id=\"weights\" font-size=\"10\" fill=\"red\">\n",
       "    <!-- Input to Hidden -->\n",
       "    <text x=\"150\" y=\"70\">0.2</text>\n",
       "    <text x=\"150\" y=\"90\">1.5</text>\n",
       "    <text x=\"150\" y=\"110\">-1.5</text>\n",
       "    <text x=\"150\" y=\"130\">0.5</text>\n",
       "    \n",
       "    <text x=\"180\" y=\"100\">-0.5</text>\n",
       "    <text x=\"180\" y=\"150\">-1.0</text>\n",
       "    <text x=\"180\" y=\"180\">2.0</text>\n",
       "    <text x=\"180\" y=\"210\">-0.5</text>\n",
       "    \n",
       "    <text x=\"230\" y=\"100\">1.0</text>\n",
       "    <text x=\"230\" y=\"160\">0.5</text>\n",
       "    <text x=\"230\" y=\"220\">-1.0</text>\n",
       "    <text x=\"230\" y=\"280\">1.5</text>\n",
       "    \n",
       "    <!-- Hidden to Output -->\n",
       "    <text x=\"400\" y=\"100\">0.3</text>\n",
       "    <text x=\"400\" y=\"140\">-0.8</text>\n",
       "    <text x=\"400\" y=\"180\">0.5</text>\n",
       "    <text x=\"400\" y=\"220\">1.0</text>\n",
       "  </g>\n",
       "  \n",
       "  <!-- Labels -->\n",
       "  <text x=\"100\" y=\"20\" text-anchor=\"middle\" font-size=\"14\" font-weight=\"bold\">Input Layer</text>\n",
       "  <text x=\"300\" y=\"20\" text-anchor=\"middle\" font-size=\"14\" font-weight=\"bold\">Hidden Layer</text>\n",
       "  <text x=\"500\" y=\"20\" text-anchor=\"middle\" font-size=\"14\" font-weight=\"bold\">Output Layer</text>\n",
       "  <text x=\"600\" y=\"160\" font-size=\"14\">Output: 0.74</text>\n",
       "</svg>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<svg width=\"800\" height=\"400\" viewBox=\"0 0 800 400\" xmlns=\"http://www.w3.org/2000/svg\">\n",
    "  <svg viewBox=\"0 0 800 400\" xmlns=\"http://www.w3.org/2000/svg\">\n",
    "  <!-- Input Layer -->\n",
    "  <g id=\"input-layer\">\n",
    "    <circle cx=\"100\" cy=\"80\" r=\"20\" fill=\"lightblue\" stroke=\"black\"/>\n",
    "    <text x=\"100\" y=\"85\" text-anchor=\"middle\" font-size=\"12\">0.5</text>\n",
    "    <circle cx=\"100\" cy=\"160\" r=\"20\" fill=\"lightblue\" stroke=\"black\"/>\n",
    "    <text x=\"100\" y=\"165\" text-anchor=\"middle\" font-size=\"12\">0.1</text>\n",
    "    <circle cx=\"100\" cy=\"240\" r=\"20\" fill=\"lightblue\" stroke=\"black\"/>\n",
    "    <text x=\"100\" y=\"245\" text-anchor=\"middle\" font-size=\"12\">0.4</text>\n",
    "  </g>\n",
    "  \n",
    "  <!-- Hidden Layer -->\n",
    "  <g id=\"hidden-layer\">\n",
    "    <circle cx=\"300\" cy=\"60\" r=\"20\" fill=\"lightgreen\" stroke=\"black\"/>\n",
    "    <text x=\"300\" y=\"65\" text-anchor=\"middle\" font-size=\"12\">0.63</text>\n",
    "    <circle cx=\"300\" cy=\"140\" r=\"20\" fill=\"lightgreen\" stroke=\"black\"/>\n",
    "    <text x=\"300\" y=\"145\" text-anchor=\"middle\" font-size=\"12\">0.74</text>\n",
    "    <circle cx=\"300\" cy=\"220\" r=\"20\" fill=\"lightgreen\" stroke=\"black\"/>\n",
    "    <text x=\"300\" y=\"225\" text-anchor=\"middle\" font-size=\"12\">0.34</text>\n",
    "    <circle cx=\"300\" cy=\"300\" r=\"20\" fill=\"lightgreen\" stroke=\"black\"/>\n",
    "    <text x=\"300\" y=\"305\" text-anchor=\"middle\" font-size=\"12\">0.77</text>\n",
    "  </g>\n",
    "  \n",
    "  <!-- Output Layer -->\n",
    "  <g id=\"output-layer\">\n",
    "    <circle cx=\"500\" cy=\"160\" r=\"20\" fill=\"lightyellow\" stroke=\"black\"/>\n",
    "    <text x=\"500\" y=\"165\" text-anchor=\"middle\" font-size=\"12\">0.74</text>\n",
    "  </g>\n",
    "  \n",
    "  <!-- Connections -->\n",
    "  <g id=\"connections\" stroke=\"gray\" stroke-width=\"1\">\n",
    "    <!-- Input to Hidden -->\n",
    "    <line x1=\"120\" y1=\"80\" x2=\"280\" y2=\"60\"/>\n",
    "    <line x1=\"120\" y1=\"80\" x2=\"280\" y2=\"140\"/>\n",
    "    <line x1=\"120\" y1=\"80\" x2=\"280\" y2=\"220\"/>\n",
    "    <line x1=\"120\" y1=\"80\" x2=\"280\" y2=\"300\"/>\n",
    "    \n",
    "    <line x1=\"120\" y1=\"160\" x2=\"280\" y2=\"60\"/>\n",
    "    <line x1=\"120\" y1=\"160\" x2=\"280\" y2=\"140\"/>\n",
    "    <line x1=\"120\" y1=\"160\" x2=\"280\" y2=\"220\"/>\n",
    "    <line x1=\"120\" y1=\"160\" x2=\"280\" y2=\"300\"/>\n",
    "    \n",
    "    <line x1=\"120\" y1=\"240\" x2=\"280\" y2=\"60\"/>\n",
    "    <line x1=\"120\" y1=\"240\" x2=\"280\" y2=\"140\"/>\n",
    "    <line x1=\"120\" y1=\"240\" x2=\"280\" y2=\"220\"/>\n",
    "    <line x1=\"120\" y1=\"240\" x2=\"280\" y2=\"300\"/>\n",
    "    \n",
    "    <!-- Hidden to Output -->\n",
    "    <line x1=\"320\" y1=\"60\" x2=\"480\" y2=\"160\"/>\n",
    "    <line x1=\"320\" y1=\"140\" x2=\"480\" y2=\"160\"/>\n",
    "    <line x1=\"320\" y1=\"220\" x2=\"480\" y2=\"160\"/>\n",
    "    <line x1=\"320\" y1=\"300\" x2=\"480\" y2=\"160\"/>\n",
    "  </g>\n",
    "  \n",
    "  <!-- Weights -->\n",
    "  <g id=\"weights\" font-size=\"10\" fill=\"red\">\n",
    "    <!-- Input to Hidden -->\n",
    "    <text x=\"150\" y=\"70\">0.2</text>\n",
    "    <text x=\"150\" y=\"90\">1.5</text>\n",
    "    <text x=\"150\" y=\"110\">-1.5</text>\n",
    "    <text x=\"150\" y=\"130\">0.5</text>\n",
    "    \n",
    "    <text x=\"180\" y=\"100\">-0.5</text>\n",
    "    <text x=\"180\" y=\"150\">-1.0</text>\n",
    "    <text x=\"180\" y=\"180\">2.0</text>\n",
    "    <text x=\"180\" y=\"210\">-0.5</text>\n",
    "    \n",
    "    <text x=\"230\" y=\"100\">1.0</text>\n",
    "    <text x=\"230\" y=\"160\">0.5</text>\n",
    "    <text x=\"230\" y=\"220\">-1.0</text>\n",
    "    <text x=\"230\" y=\"280\">1.5</text>\n",
    "    \n",
    "    <!-- Hidden to Output -->\n",
    "    <text x=\"400\" y=\"100\">0.3</text>\n",
    "    <text x=\"400\" y=\"140\">-0.8</text>\n",
    "    <text x=\"400\" y=\"180\">0.5</text>\n",
    "    <text x=\"400\" y=\"220\">1.0</text>\n",
    "  </g>\n",
    "  \n",
    "  <!-- Labels -->\n",
    "  <text x=\"100\" y=\"20\" text-anchor=\"middle\" font-size=\"14\" font-weight=\"bold\">Input Layer</text>\n",
    "  <text x=\"300\" y=\"20\" text-anchor=\"middle\" font-size=\"14\" font-weight=\"bold\">Hidden Layer</text>\n",
    "  <text x=\"500\" y=\"20\" text-anchor=\"middle\" font-size=\"14\" font-weight=\"bold\">Output Layer</text>\n",
    "  <text x=\"600\" y=\"160\" font-size=\"14\">Output: 0.74</text>\n",
    "</svg>\n",
    "</svg>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452e52ab-79e4-4d91-97d4-de47e6583488",
   "metadata": {},
   "source": [
    "### Feedforward Calculation\n",
    "\n",
    "Let's break down the `feedforward` function step by step:\n",
    "\n",
    "1. **Initialization**: Set the input vector `a` as the initial activation.\n",
    "\n",
    "    ```python\n",
    "    a = input_vector\n",
    "    ```\n",
    "\n",
    "2. **First Layer Calculation (Input to Hidden)**:\n",
    "   - Compute the weighted input $z_1$ for the hidden layer:\n",
    "     \\begin{equation}\n",
    "     z_1 = w_1 \\cdot a + b_1\n",
    "     \\end{equation}\n",
    "     Substituting the values:\n",
    "     $$\n",
    "     z_1 = \\begin{bmatrix}\n",
    "     0.2 & -0.5 & 1.0 \\\\\n",
    "     1.5 & -1.0 & 0.5 \\\\\n",
    "     -1.5 & 2.0 & -1.0 \\\\\n",
    "     0.5 & -0.5 & 1.5 \n",
    "     \\end{bmatrix} \\cdot \\begin{bmatrix}\n",
    "     0.5 \\\\\n",
    "     0.1 \\\\\n",
    "     0.4 \n",
    "     \\end{bmatrix} + \\begin{bmatrix}\n",
    "     0.1 \\\\\n",
    "     0.2 \\\\\n",
    "     0.3 \\\\\n",
    "     0.4 \n",
    "     \\end{bmatrix}\n",
    "     $$\n",
    "     \n",
    "     Calculate the dot product:\n",
    "     \n",
    "     $$\n",
    "     w_1 \\cdot a = \\begin{bmatrix}\n",
    "     0.2 \\cdot 0.5 + -0.5 \\cdot 0.1 + 1.0 \\cdot 0.4 \\\\\n",
    "     1.5 \\cdot 0.5 + -1.0 \\cdot 0.1 + 0.5 \\cdot 0.4 \\\\\n",
    "     -1.5 \\cdot 0.5 + 2.0 \\cdot 0.1 + -1.0 \\cdot 0.4 \\\\\n",
    "     0.5 \\cdot 0.5 + -0.5 \\cdot 0.1 + 1.5 \\cdot 0.4\n",
    "     \\end{bmatrix} = \\begin{bmatrix}\n",
    "     0.1 + -0.05 + 0.4 \\\\\n",
    "     0.75 + -0.1 + 0.2 \\\\\n",
    "     -0.75 + 0.2 + -0.4 \\\\\n",
    "     0.25 + -0.05 + 0.6\n",
    "     \\end{bmatrix} = \\begin{bmatrix}\n",
    "     0.45 \\\\\n",
    "     0.85 \\\\\n",
    "     -0.95 \\\\\n",
    "     0.8 \n",
    "     \\end{bmatrix}\n",
    "     $$\n",
    "     Adding the bias:\n",
    "     $$\n",
    "     z_1 = \\begin{bmatrix}\n",
    "     0.45 \\\\\n",
    "     0.85 \\\\\n",
    "     -0.95 \\\\\n",
    "     0.8 \n",
    "     \\end{bmatrix} + \\begin{bmatrix}\n",
    "     0.1 \\\\\n",
    "     0.2 \\\\\n",
    "     0.3 \\\\\n",
    "     0.4 \n",
    "     \\end{bmatrix} = \\begin{bmatrix}\n",
    "     0.55 \\\\\n",
    "     1.05 \\\\\n",
    "     -0.65 \\\\\n",
    "     1.2 \n",
    "     \\end{bmatrix}\n",
    "     $$\n",
    "\n",
    "   - Apply the sigmoid activation function to get the activation $a_1$ of the hidden layer:\n",
    "     $$\n",
    "     a_1 = \\sigma(z_1) = \\sigma\\left(\\begin{bmatrix}\n",
    "     0.55 \\\\\n",
    "     1.05 \\\\\n",
    "     -0.65 \\\\\n",
    "     1.2 \n",
    "     \\end{bmatrix}\\right) = \\begin{bmatrix}\n",
    "     \\sigma(0.55) \\\\\n",
    "     \\sigma(1.05) \\\\\n",
    "     \\sigma(-0.65) \\\\\n",
    "     \\sigma(1.2)\n",
    "     \\end{bmatrix} = \\begin{bmatrix}\n",
    "     0.63413559 \\\\\n",
    "     0.7407749 \\\\\n",
    "     0.34298954 \\\\\n",
    "     0.76852478\n",
    "     \\end{bmatrix}\n",
    "     $$\n",
    "\n",
    "3. **Second Layer Calculation (Hidden to Output)**:\n",
    "   - Compute the weighted input $z_2$ for the output layer:\n",
    "     $$\n",
    "     z_2 = w_2 \\cdot a_1 + b_2\n",
    "     $$\n",
    "     \n",
    "     Substituting the values:\n",
    "     \n",
    "     $$\n",
    "     z_2 = \\begin{bmatrix}\n",
    "     0.3 & -0.8 & 0.5 & 1.0\n",
    "     \\end{bmatrix} \\cdot \\begin{bmatrix}\n",
    "     0.63413559 \\\\\n",
    "     0.7407749 \\\\\n",
    "     0.34298954 \\\\\n",
    "     0.76852478\n",
    "     \\end{bmatrix} + \\begin{bmatrix}\n",
    "     0.5\n",
    "     \\end{bmatrix}\n",
    "     $$\n",
    "     Calculate the dot product:\n",
    "     $$\n",
    "     w_2 \\cdot a_1 = 0.3 \\cdot 0.63413559 + -0.8 \\cdot 0.7407749 + 0.5 \\cdot 0.34298954 + 1.0 \\cdot 0.76852478 = 0.19024068 + -0.59261992 + 0.17149477 + 0.76852478 = 0.53764031\n",
    "     $$\n",
    "     Adding the bias:\n",
    "     $$\n",
    "     z_2 = 0.53764031 + 0.5 = 1.03764031\n",
    "     $$\n",
    "\n",
    "   - Apply the sigmoid activation function to get the activation `a2` of the output layer:\n",
    "     $$\n",
    "     a_2 = \\sigma(z_2) = \\sigma(1.03764031) = 0.73841914\n",
    "     $$\n",
    "\n",
    "### Final Output\n",
    "So, given the input vector `[[0.5], [0.1], [0.4]]`, the network produces an output of `0.73841914`.\n",
    "\n",
    "### Summary\n",
    "- **Input Layer**: Receives the input `[[0.5], [0.1], [0.4]]`.\n",
    "- **Hidden Layer**: \n",
    "  - Computes $z_1$ using weights and biases.\n",
    "  - Applies the sigmoid function to $z_1$ to get $a_1$.\n",
    "- **Output Layer**:\n",
    "  - Computes $z_2$ using weights and biases.\n",
    "  - Applies the sigmoid function to $z_2$ to get $a_2$.\n",
    "\n",
    "This step-by-step example demonstrates how the `feedforward` function processes an input through the layers of the network to produce the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2dca4acd-2bb2-465d-9c8b-a5d00fb877f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network Feedforward Result:\n",
      "Input: [[0.5 0.1 0.4]]\n",
      "Output: 0.73839445\n",
      "\n",
      "Comparison:\n",
      "Manual calculation result: 0.73841914\n",
      "Network feedforward result: 0.73839445\n",
      "Difference: 0.00002469\n"
     ]
    }
   ],
   "source": [
    "# Create a network with the same structure as our previous example\n",
    "net = Network([3, 4, 1])\n",
    "\n",
    "# Set the weights and biases to match our previous example\n",
    "net.weights = [\n",
    "    np.array([[0.2, -0.5, 1.0],\n",
    "              [1.5, -1.0, 0.5],\n",
    "              [-1.5, 2.0, -1.0],\n",
    "              [0.5, -0.5, 1.5]]),\n",
    "    np.array([[0.3, -0.8, 0.5, 1.0]])\n",
    "]\n",
    "\n",
    "net.biases = [\n",
    "    np.array([[0.1], [0.2], [0.3], [0.4]]),\n",
    "    np.array([[0.5]])\n",
    "]\n",
    "\n",
    "# Input vector\n",
    "input_vector = np.array([[0.5], [0.1], [0.4]])\n",
    "\n",
    "# Use the feedforward method\n",
    "output = net.feedforward(input_vector)\n",
    "\n",
    "print(\"Network Feedforward Result:\")\n",
    "print(f\"Input: {input_vector.T}\")\n",
    "print(f\"Output: {output[0][0]:.8f}\")\n",
    "\n",
    "# Compare with our previous manual calculation\n",
    "print(\"\\nComparison:\")\n",
    "print(f\"Manual calculation result: 0.73841914\")\n",
    "print(f\"Network feedforward result: {output[0][0]:.8f}\")\n",
    "print(f\"Difference: {abs(0.73841914 - output[0][0]):.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87a8e492-6c75-4ab6-8c6c-72703917e14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network Feedforward Result:\n",
      "Input: [[0.5 0.1 0.4]]\n",
      "Output: 0.78564259\n"
     ]
    }
   ],
   "source": [
    "# Create a network with the same structure as our previous example\n",
    "# The weights and biases are initialized by the Network object.\n",
    "net = Network([3, 4, 1])\n",
    "\n",
    "# Input vector\n",
    "input_vector = np.array([[0.5], [0.1], [0.4]])\n",
    "\n",
    "# Use the feedforward method\n",
    "output = net.feedforward(input_vector)\n",
    "\n",
    "print(\"Network Feedforward Result:\")\n",
    "print(f\"Input: {input_vector.T}\")\n",
    "print(f\"Output: {output[0][0]:.8f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f555ea10-0356-4490-8aa0-52f9478ebd9f",
   "metadata": {},
   "source": [
    "## Understand `SGD` function of `Network` object\n",
    "We'll demonstrate the workflow of the `SGD` function using a hypothetical dataset for the neural network structure described in the notebook (3 input neurons, 4 hidden neurons, 1 output neuron)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac89071-b655-4836-a239-c17ccb1f7ab2",
   "metadata": {},
   "source": [
    "Let's start by creating a hypothetical dataset and then go through the SGD function step by step.\n",
    "### Step 1: Create a hypothetical dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83df0260-ac5e-40a4-b7a1-400db4c81437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:\n",
      "Sample 1: Input [0.23984301 0.36823297 0.73021352], Output 0.9632\n",
      "Sample 2: Input [0.84115672 0.84562775 0.57006856], Output 0.4770\n",
      "Sample 3: Input [0.08615032 0.12027176 0.32592854], Output 0.2486\n",
      "Sample 4: Input [0.25883698 0.08358081 0.40747107], Output 0.6444\n",
      "Sample 5: Input [0.91296901 0.2102162  0.80432432], Output 0.8701\n",
      "Sample 6: Input [0.08612347 0.385473   0.44872146], Output 0.2585\n",
      "Sample 7: Input [0.72153488 0.63716517 0.61898078], Output 0.8623\n",
      "Sample 8: Input [0.36620078 0.2239276  0.59579017], Output 0.9833\n",
      "Sample 9: Input [0.95651739 0.31186137 0.41393432], Output 0.5588\n",
      "Sample 10: Input [0.50652544 0.35704097 0.54453999], Output 0.0947\n",
      "Sample 11: Input [0.46581827 0.33754085 0.21991484], Output 0.2355\n",
      "Sample 12: Input [0.95287591 0.02280938 0.92499575], Output 0.8820\n"
     ]
    }
   ],
   "source": [
    "# Create 10 random input samples\n",
    "inputs = np.random.rand(12, 3)\n",
    "# Create corresponding random output samples (between 0 and 1)\n",
    "outputs = np.random.rand(12, 1)\n",
    "\n",
    "# Combine inputs and outputs into training data\n",
    "training_data = list(zip(inputs, outputs))\n",
    "\n",
    "print(\"Training Data:\")\n",
    "for i, (x, y) in enumerate(training_data):\n",
    "    print(f\"Sample {i+1}: Input {x}, Output {y[0]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383b16a4-e652-4b67-8bc4-5cec7f106d3a",
   "metadata": {},
   "source": [
    "### Step 2: Initialize the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac33575d-7002-4f1a-966a-09830a80cdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Network([3, 4, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7281f646-a401-4e20-9749-20548122eeff",
   "metadata": {},
   "source": [
    "### Step 3: Set up SGD parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9c43421-32cb-4665-acc6-baad2f6be6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "mini_batch_size = 4\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314f4a23-2b15-4645-a3a7-e2e1d0998e79",
   "metadata": {},
   "source": [
    "Let's walk through the `SGD` function step by step:\n",
    "\n",
    "```python\n",
    "def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n",
    "        \"\"\"\n",
    "        Train the neural network using mini-batch stochastic gradient descent.\n",
    "        \n",
    "        Parameters:\n",
    "        training_data (list): A list of tuples '(x, y)' representing the training inputs\n",
    "                              and the desired outputs.\n",
    "        epochs (int): The number of epochs to train for.\n",
    "        mini_batch_size (int): The size of the mini-batches to use when sampling.\n",
    "        eta (float): The learning rate.\n",
    "        test_data (list, optional): If provided, the network will be evaluated\n",
    "                                    against the test data after each epoch, and\n",
    "                                    partial progress will be printed out.\n",
    "        \"\"\"\n",
    "        if test_data: n_test = len(test_data)\n",
    "        n = len(training_data)\n",
    "        for j in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [training_data[k:k+mini_batch_size] for k in range(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            if test_data:\n",
    "                print(f\"Epoch {j}: {self.evaluate(test_data)} / {n_test}\")\n",
    "            else:\n",
    "                print(f\"Epoch {j} complete\")\n",
    "```\n",
    "\n",
    "Let's break this down:\n",
    "\n",
    "> 1. The function takes training data, number of epochs, mini batch size, learning rate (eta), and optimal test data as input/arguments.\n",
    "> 2. It iterates through the number of epochs specified. For instance, if the number of epochs is 20, then it iterates through the function 10 times, each with different mini batches.\n",
    "> 3. At the beginning of each epoch, the function shuffles the training data for mini-batch selection.\n",
    "> 4. After shuffling, the entire training data is split into mini batches with each batch size is specified by mini-batch size. For instance, if we have 100 training examples and mini batch size is 20. Then the entire training data is splitted into 5 mini batches, each containing 20 training examples.\n",
    "> 5. For each mini batch, the function calls another function `update_mini_batch` to update weights and biases of the model.\n",
    "> 6. If test data is provided, it evaluates the performance of the model with the updated weights and biases.\n",
    "> 7. After processing all mini batches within an epoch, it prints the epoch completion status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b0ba1605-8dee-49ee-8b20-31c5804a5af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of training examples in the training data is 12.\n",
      "Training Data:\n",
      "Sample 1: Input [0.23984301 0.36823297 0.73021352], Output 0.9632\n",
      "Sample 2: Input [0.84115672 0.84562775 0.57006856], Output 0.4770\n",
      "Sample 3: Input [0.08615032 0.12027176 0.32592854], Output 0.2486\n",
      "Sample 4: Input [0.25883698 0.08358081 0.40747107], Output 0.6444\n",
      "Sample 5: Input [0.91296901 0.2102162  0.80432432], Output 0.8701\n",
      "Sample 6: Input [0.08612347 0.385473   0.44872146], Output 0.2585\n",
      "Sample 7: Input [0.72153488 0.63716517 0.61898078], Output 0.8623\n",
      "Sample 8: Input [0.36620078 0.2239276  0.59579017], Output 0.9833\n",
      "Sample 9: Input [0.95651739 0.31186137 0.41393432], Output 0.5588\n",
      "Sample 10: Input [0.50652544 0.35704097 0.54453999], Output 0.0947\n",
      "Sample 11: Input [0.46581827 0.33754085 0.21991484], Output 0.2355\n",
      "Sample 12: Input [0.95287591 0.02280938 0.92499575], Output 0.8820\n",
      "\n",
      "Epoch 1:\n",
      "Shuffled data:\n",
      "Sample 1: Input [0.23984301 0.36823297 0.73021352], Output 0.9632\n",
      "Sample 2: Input [0.84115672 0.84562775 0.57006856], Output 0.4770\n",
      "Sample 3: Input [0.72153488 0.63716517 0.61898078], Output 0.8623\n",
      "Sample 4: Input [0.95651739 0.31186137 0.41393432], Output 0.5588\n",
      "Sample 5: Input [0.08615032 0.12027176 0.32592854], Output 0.2486\n",
      "Sample 6: Input [0.91296901 0.2102162  0.80432432], Output 0.8701\n",
      "Sample 7: Input [0.36620078 0.2239276  0.59579017], Output 0.9833\n",
      "Sample 8: Input [0.50652544 0.35704097 0.54453999], Output 0.0947\n",
      "Sample 9: Input [0.25883698 0.08358081 0.40747107], Output 0.6444\n",
      "Sample 10: Input [0.46581827 0.33754085 0.21991484], Output 0.2355\n",
      "Sample 11: Input [0.08612347 0.385473   0.44872146], Output 0.2585\n",
      "Sample 12: Input [0.95287591 0.02280938 0.92499575], Output 0.8820\n",
      "\n",
      "Number of mini-batches: 3\n",
      "\n",
      "Mini-batch 1:\n",
      "Input [0.23984301 0.36823297 0.73021352], Output 0.9632\n",
      "Input [0.84115672 0.84562775 0.57006856], Output 0.4770\n",
      "Input [0.72153488 0.63716517 0.61898078], Output 0.8623\n",
      "Input [0.95651739 0.31186137 0.41393432], Output 0.5588\n",
      "Updating network weights and biases...\n",
      "\n",
      "Mini-batch 2:\n",
      "Input [0.08615032 0.12027176 0.32592854], Output 0.2486\n",
      "Input [0.91296901 0.2102162  0.80432432], Output 0.8701\n",
      "Input [0.36620078 0.2239276  0.59579017], Output 0.9833\n",
      "Input [0.50652544 0.35704097 0.54453999], Output 0.0947\n",
      "Updating network weights and biases...\n",
      "\n",
      "Mini-batch 3:\n",
      "Input [0.25883698 0.08358081 0.40747107], Output 0.6444\n",
      "Input [0.46581827 0.33754085 0.21991484], Output 0.2355\n",
      "Input [0.08612347 0.385473   0.44872146], Output 0.2585\n",
      "Input [0.95287591 0.02280938 0.92499575], Output 0.8820\n",
      "Updating network weights and biases...\n",
      "Epoch 1 complete\n",
      "\n",
      "Epoch 2:\n",
      "Shuffled data:\n",
      "Sample 1: Input [0.36620078 0.2239276  0.59579017], Output 0.9833\n",
      "Sample 2: Input [0.84115672 0.84562775 0.57006856], Output 0.4770\n",
      "Sample 3: Input [0.95287591 0.02280938 0.92499575], Output 0.8820\n",
      "Sample 4: Input [0.08612347 0.385473   0.44872146], Output 0.2585\n",
      "Sample 5: Input [0.08615032 0.12027176 0.32592854], Output 0.2486\n",
      "Sample 6: Input [0.91296901 0.2102162  0.80432432], Output 0.8701\n",
      "Sample 7: Input [0.25883698 0.08358081 0.40747107], Output 0.6444\n",
      "Sample 8: Input [0.72153488 0.63716517 0.61898078], Output 0.8623\n",
      "Sample 9: Input [0.46581827 0.33754085 0.21991484], Output 0.2355\n",
      "Sample 10: Input [0.95651739 0.31186137 0.41393432], Output 0.5588\n",
      "Sample 11: Input [0.50652544 0.35704097 0.54453999], Output 0.0947\n",
      "Sample 12: Input [0.23984301 0.36823297 0.73021352], Output 0.9632\n",
      "\n",
      "Number of mini-batches: 3\n",
      "\n",
      "Mini-batch 1:\n",
      "Input [0.36620078 0.2239276  0.59579017], Output 0.9833\n",
      "Input [0.84115672 0.84562775 0.57006856], Output 0.4770\n",
      "Input [0.95287591 0.02280938 0.92499575], Output 0.8820\n",
      "Input [0.08612347 0.385473   0.44872146], Output 0.2585\n",
      "Updating network weights and biases...\n",
      "\n",
      "Mini-batch 2:\n",
      "Input [0.08615032 0.12027176 0.32592854], Output 0.2486\n",
      "Input [0.91296901 0.2102162  0.80432432], Output 0.8701\n",
      "Input [0.25883698 0.08358081 0.40747107], Output 0.6444\n",
      "Input [0.72153488 0.63716517 0.61898078], Output 0.8623\n",
      "Updating network weights and biases...\n",
      "\n",
      "Mini-batch 3:\n",
      "Input [0.46581827 0.33754085 0.21991484], Output 0.2355\n",
      "Input [0.95651739 0.31186137 0.41393432], Output 0.5588\n",
      "Input [0.50652544 0.35704097 0.54453999], Output 0.0947\n",
      "Input [0.23984301 0.36823297 0.73021352], Output 0.9632\n",
      "Updating network weights and biases...\n",
      "Epoch 2 complete\n",
      "\n",
      "Epoch 3:\n",
      "Shuffled data:\n",
      "Sample 1: Input [0.08612347 0.385473   0.44872146], Output 0.2585\n",
      "Sample 2: Input [0.25883698 0.08358081 0.40747107], Output 0.6444\n",
      "Sample 3: Input [0.50652544 0.35704097 0.54453999], Output 0.0947\n",
      "Sample 4: Input [0.36620078 0.2239276  0.59579017], Output 0.9833\n",
      "Sample 5: Input [0.46581827 0.33754085 0.21991484], Output 0.2355\n",
      "Sample 6: Input [0.91296901 0.2102162  0.80432432], Output 0.8701\n",
      "Sample 7: Input [0.23984301 0.36823297 0.73021352], Output 0.9632\n",
      "Sample 8: Input [0.72153488 0.63716517 0.61898078], Output 0.8623\n",
      "Sample 9: Input [0.08615032 0.12027176 0.32592854], Output 0.2486\n",
      "Sample 10: Input [0.95287591 0.02280938 0.92499575], Output 0.8820\n",
      "Sample 11: Input [0.84115672 0.84562775 0.57006856], Output 0.4770\n",
      "Sample 12: Input [0.95651739 0.31186137 0.41393432], Output 0.5588\n",
      "\n",
      "Number of mini-batches: 3\n",
      "\n",
      "Mini-batch 1:\n",
      "Input [0.08612347 0.385473   0.44872146], Output 0.2585\n",
      "Input [0.25883698 0.08358081 0.40747107], Output 0.6444\n",
      "Input [0.50652544 0.35704097 0.54453999], Output 0.0947\n",
      "Input [0.36620078 0.2239276  0.59579017], Output 0.9833\n",
      "Updating network weights and biases...\n",
      "\n",
      "Mini-batch 2:\n",
      "Input [0.46581827 0.33754085 0.21991484], Output 0.2355\n",
      "Input [0.91296901 0.2102162  0.80432432], Output 0.8701\n",
      "Input [0.23984301 0.36823297 0.73021352], Output 0.9632\n",
      "Input [0.72153488 0.63716517 0.61898078], Output 0.8623\n",
      "Updating network weights and biases...\n",
      "\n",
      "Mini-batch 3:\n",
      "Input [0.08615032 0.12027176 0.32592854], Output 0.2486\n",
      "Input [0.95287591 0.02280938 0.92499575], Output 0.8820\n",
      "Input [0.84115672 0.84562775 0.57006856], Output 0.4770\n",
      "Input [0.95651739 0.31186137 0.41393432], Output 0.5588\n",
      "Updating network weights and biases...\n",
      "Epoch 3 complete\n"
     ]
    }
   ],
   "source": [
    "def simulate_sgd(training_data, epochs, mini_batch_size, eta):\n",
    "    n = len(training_data)\n",
    "    print(f\"The total number of training examples in the training data is {n}.\")\n",
    "    print(\"Training Data:\")\n",
    "    for i, (x, y) in enumerate(training_data):\n",
    "        print(f\"Sample {i+1}: Input {x}, Output {y[0]:.4f}\")\n",
    "        \n",
    "    for j in range(epochs):\n",
    "        print(f\"\\nEpoch {j+1}:\")\n",
    "        random.shuffle(training_data)\n",
    "        print(\"Shuffled data:\")\n",
    "        for i, (x, y) in enumerate(training_data):\n",
    "            print(f\"Sample {i+1}: Input {x}, Output {y[0]:.4f}\")\n",
    "        \n",
    "        mini_batches = [training_data[k:k+mini_batch_size] for k in range(0, n, mini_batch_size)]\n",
    "        print(f\"\\nNumber of mini-batches: {len(mini_batches)}\")\n",
    "        \n",
    "        for i, mini_batch in enumerate(mini_batches):\n",
    "            print(f\"\\nMini-batch {i+1}:\")\n",
    "            for x, y in mini_batch:\n",
    "                print(f\"Input {x}, Output {y[0]:.4f}\")\n",
    "            print(\"Updating network weights and biases...\")\n",
    "        \n",
    "        print(f\"Epoch {j+1} complete\")\n",
    "\n",
    "# Run the simulation\n",
    "simulate_sgd(training_data, epochs, mini_batch_size, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0b0e52d4-205a-495e-9213-6ffa156b2a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_update_mini_batch(net, mini_batch, eta):\n",
    "    print(\"\\nSimulating update_mini_batch:\")\n",
    "    print(f\"Mini-batch size: {len(mini_batch)}\")\n",
    "    print(f\"Learning rate (eta): {eta}\")\n",
    "    for b in net.biases:\n",
    "        print(f\"The shape of the biases: {b.shape}\")\n",
    "    for w in net.weights:\n",
    "        print(f\"The shape of the weights: {w.shape}\")\n",
    "    \n",
    "    nabla_b = [np.zeros(b.shape) for b in net.biases]\n",
    "    nabla_w = [np.zeros(w.shape) for w in net.weights]\n",
    "    \n",
    "    for i, (x, y) in enumerate(mini_batch):\n",
    "        print(f\"\\nProcessing example {i+1}:\")\n",
    "        print(f\"Input: {x.flatten()}\")\n",
    "        print(f\"Target output: {y.flatten()}\")\n",
    "        \n",
    "        # Forward propagation\n",
    "        activation = x\n",
    "        for j, (b, w) in enumerate(zip(net.biases, net.weights)):\n",
    "            activation = sigmoid(np.dot(w, activation) + b)\n",
    "            print(f\"Layer {j+1} output: {activation.flatten()}\")\n",
    "        \n",
    "        print(f\"Network output: {activation.flatten()}\")\n",
    "        \n",
    "        # Backpropagation\n",
    "        delta_nabla_b, delta_nabla_w = net.backprop(x, y)\n",
    "        \n",
    "        print(\"\\nGradients:\")\n",
    "        for j, (dnb, dnw) in enumerate(zip(delta_nabla_b, delta_nabla_w)):\n",
    "            print(f\"Layer {j+1}:\")\n",
    "            print(f\"  Bias gradients: {dnb.flatten()}\")\n",
    "            print(f\"  Weight gradients shape: {dnw.shape}\")\n",
    "        \n",
    "        nabla_b = [nb + dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "        nabla_w = [nw + dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "    \n",
    "    print(\"\\nUpdating weights and biases:\")\n",
    "    for i, (w, b, nw, nb) in enumerate(zip(net.weights, net.biases, nabla_w, nabla_b)):\n",
    "        print(f\"\\nLayer {i+1}:\")\n",
    "        w_update = (eta / len(mini_batch)) * nw\n",
    "        b_update = (eta / len(mini_batch)) * nb\n",
    "        print(f\"Weight update magnitude: {np.linalg.norm(w_update)}\")\n",
    "        print(f\"Bias update magnitude: {np.linalg.norm(b_update)}\")\n",
    "        net.weights[i] = w - w_update\n",
    "        net.biases[i] = b - b_update\n",
    "\n",
    "def simulate_sgd(training_data, epochs, mini_batch_size, eta):\n",
    "    net = Network([3, 4, 1])\n",
    "    n = len(training_data)\n",
    "    \n",
    "    for j in range(epochs):\n",
    "        print(f\"\\nEpoch {j+1}:\")\n",
    "        random.shuffle(training_data)\n",
    "        mini_batches = [training_data[k:k+mini_batch_size] for k in range(0, n, mini_batch_size)]\n",
    "        \n",
    "        for i, mini_batch in enumerate(mini_batches):\n",
    "            print(f\"\\nProcessing mini-batch {i+1}:\")\n",
    "            simulate_update_mini_batch(net, mini_batch, eta)\n",
    "        \n",
    "        print(f\"\\nEpoch {j+1} complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1d19f2-ac98-49ae-a635-4d187888168e",
   "metadata": {},
   "source": [
    "Let's break this down:\n",
    "\n",
    ">1. `simulate_sgd` works in the similar way as perviously explained in this notebook.\n",
    ">2. `simulate_update_mini_batch` function starts with initializing gradients with zeros. This initialization corresponds to layers. For instance, if we have three-layer network with one input layer, one hidden layer and one output layer, then the initialization corresponds to the connection between input layer and hidden layer, and between hidden layer and output layer.\n",
    ">3. Next step involves iterating through each training example in the training data. For each training example in the current mini-batch:\n",
    "    - Implement forward propagation to compute neuron output using sigmpoid activation function and current network weights and biases, which is stored in the `activation` variable.\n",
    "    - Implement backpropagation to compute `delta_nabla_b` and `delta_nabla_w`. `delta_nabla_b` and `delta_nabla_w` are partial derivatives of the cost function with respect to biases and weights, respectively, for a single training example.\n",
    "    - The gradients are accumulated in `nabla_b` and `nabla_w`\n",
    ">4. After processing all training examples in a mini-batch:\n",
    "    - `w_update` and `b_update` represents the average of accumulated gradient, adjusted by the learning rate.\n",
    "    - `weights` and `biases` are then updated based on the `w_update` and `b_update`.\n",
    "    \n",
    "Two important points here:\n",
    "1. Single training example: The backprop method computes these gradients for one training example at a time. This is why we accumulate these gradients over the mini-batch.\n",
    "2. Iteration specificity: These gradients are specific to the current state of the network (current weights and biases) and the particular training example being processed in that iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5dfab283-7858-41fe-8733-dbe55e9be85d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 1th training example is: (array([[0.37454012],\n",
      "       [0.95071431],\n",
      "       [0.73199394]]), array([[0.59865848]]))\n",
      "The 2th training example is: (array([[0.15601864],\n",
      "       [0.15599452],\n",
      "       [0.05808361]]), array([[0.86617615]]))\n",
      "The 3th training example is: (array([[0.60111501],\n",
      "       [0.70807258],\n",
      "       [0.02058449]]), array([[0.96990985]]))\n",
      "The 4th training example is: (array([[0.83244264],\n",
      "       [0.21233911],\n",
      "       [0.18182497]]), array([[0.18340451]]))\n",
      "The 5th training example is: (array([[0.30424224],\n",
      "       [0.52475643],\n",
      "       [0.43194502]]), array([[0.29122914]]))\n",
      "The 6th training example is: (array([[0.61185289],\n",
      "       [0.13949386],\n",
      "       [0.29214465]]), array([[0.36636184]]))\n",
      "The 7th training example is: (array([[0.45606998],\n",
      "       [0.78517596],\n",
      "       [0.19967378]]), array([[0.51423444]]))\n",
      "The 8th training example is: (array([[0.59241457],\n",
      "       [0.04645041],\n",
      "       [0.60754485]]), array([[0.17052412]]))\n",
      "The 9th training example is: (array([[0.06505159],\n",
      "       [0.94888554],\n",
      "       [0.96563203]]), array([[0.80839735]]))\n",
      "The 10th training example is: (array([[0.30461377],\n",
      "       [0.09767211],\n",
      "       [0.68423303]]), array([[0.44015249]]))\n",
      "The 11th training example is: (array([[0.12203823],\n",
      "       [0.49517691],\n",
      "       [0.03438852]]), array([[0.9093204]]))\n",
      "The 12th training example is: (array([[0.25877998],\n",
      "       [0.66252228],\n",
      "       [0.31171108]]), array([[0.52006802]]))\n",
      "\n",
      "Epoch 1:\n",
      "\n",
      "Processing mini-batch 1:\n",
      "\n",
      "Simulating update_mini_batch:\n",
      "Mini-batch size: 4\n",
      "Learning rate (eta): 0.1\n",
      "The shape of the biases: (4, 1)\n",
      "The shape of the biases: (1, 1)\n",
      "The shape of the weights: (4, 3)\n",
      "The shape of the weights: (1, 4)\n",
      "\n",
      "Processing example 1:\n",
      "Input: [0.61185289 0.13949386 0.29214465]\n",
      "Target output: [0.36636184]\n",
      "Layer 1 output: [0.12686567 0.54062683 0.39950054 0.5358677 ]\n",
      "Layer 2 output: [0.4019801]\n",
      "Network output: [0.4019801]\n",
      "\n",
      "Gradients:\n",
      "Layer 1:\n",
      "  Bias gradients: [ 0.00088328 -0.00178456 -0.00063515  0.00070545]\n",
      "  Weight gradients shape: (4, 3)\n",
      "Layer 2:\n",
      "  Bias gradients: [0.00856235]\n",
      "  Weight gradients shape: (1, 4)\n",
      "\n",
      "Processing example 2:\n",
      "Input: [0.12203823 0.49517691 0.03438852]\n",
      "Target output: [0.9093204]\n",
      "Layer 1 output: [0.10699438 0.66282853 0.66162513 0.60518107]\n",
      "Layer 2 output: [0.35976778]\n",
      "Network output: [0.35976778]\n",
      "\n",
      "Gradients:\n",
      "Layer 1:\n",
      "  Bias gradients: [-0.01126327  0.02374082  0.00876266 -0.01001903]\n",
      "  Weight gradients shape: (4, 3)\n",
      "Layer 2:\n",
      "  Bias gradients: [-0.12658116]\n",
      "  Weight gradients shape: (1, 4)\n",
      "\n",
      "Processing example 3:\n",
      "Input: [0.06505159 0.94888554 0.96563203]\n",
      "Target output: [0.80839735]\n",
      "Layer 1 output: [0.03089358 0.81784171 0.636325   0.84596659]\n",
      "Layer 2 output: [0.33408945]\n",
      "Network output: [0.33408945]\n",
      "\n",
      "Gradients:\n",
      "Layer 1:\n",
      "  Bias gradients: [-0.00294211  0.01319264  0.00755072 -0.00455492]\n",
      "  Weight gradients shape: (4, 3)\n",
      "Layer 2:\n",
      "  Bias gradients: [-0.10552103]\n",
      "  Weight gradients shape: (1, 4)\n",
      "\n",
      "Processing example 4:\n",
      "Input: [0.45606998 0.78517596 0.19967378]\n",
      "Target output: [0.51423444]\n",
      "Layer 1 output: [0.058959   0.70794334 0.52796304 0.63385286]\n",
      "Layer 2 output: [0.35248365]\n",
      "Network output: [0.35248365]\n",
      "\n",
      "Gradients:\n",
      "Layer 1:\n",
      "  Bias gradients: [-0.00190755  0.00640584  0.00284494 -0.00283827]\n",
      "  Weight gradients shape: (4, 3)\n",
      "Layer 2:\n",
      "  Bias gradients: [-0.03691783]\n",
      "  Weight gradients shape: (1, 4)\n",
      "\n",
      "Updating weights and biases:\n",
      "\n",
      "Layer 1:\n",
      "Weight update magnitude: 0.0010024969008852994\n",
      "Bias update magnitude: 0.0012700793862889046\n",
      "\n",
      "Layer 2:\n",
      "Weight update magnitude: 0.00786835930297601\n",
      "Bias update magnitude: 0.006511441729283464\n",
      "\n",
      "Processing mini-batch 2:\n",
      "\n",
      "Simulating update_mini_batch:\n",
      "Mini-batch size: 4\n",
      "Learning rate (eta): 0.1\n",
      "The shape of the biases: (4, 1)\n",
      "The shape of the biases: (1, 1)\n",
      "The shape of the weights: (4, 3)\n",
      "The shape of the weights: (1, 4)\n",
      "\n",
      "Processing example 1:\n",
      "Input: [0.30424224 0.52475643 0.43194502]\n",
      "Target output: [0.29122914]\n",
      "Layer 1 output: [0.07543637 0.6809746  0.55108587 0.67532663]\n",
      "Layer 2 output: [0.36624823]\n",
      "Network output: [0.36624823]\n",
      "\n",
      "Gradients:\n",
      "Layer 1:\n",
      "  Bias gradients: [ 0.00113155 -0.00315653 -0.00131402  0.00128237]\n",
      "  Weight gradients shape: (4, 3)\n",
      "Layer 2:\n",
      "  Bias gradients: [0.01741272]\n",
      "  Weight gradients shape: (1, 4)\n",
      "\n",
      "Processing example 2:\n",
      "Input: [0.25877998 0.66252228 0.31171108]\n",
      "Target output: [0.52006802]\n",
      "Layer 1 output: [0.06853192 0.70753995 0.59294136 0.67337276]\n",
      "Layer 2 output: [0.35655004]\n",
      "Network output: [0.35655004]\n",
      "\n",
      "Gradients:\n",
      "Layer 1:\n",
      "  Bias gradients: [-0.00223126  0.00647747  0.002762   -0.00277136]\n",
      "  Weight gradients shape: (4, 3)\n",
      "Layer 2:\n",
      "  Bias gradients: [-0.03751464]\n",
      "  Weight gradients shape: (1, 4)\n",
      "\n",
      "Processing example 3:\n",
      "Input: [0.37454012 0.95071431 0.73199394]\n",
      "Target output: [0.59865848]\n",
      "Layer 1 output: [0.03315503 0.78225184 0.52583052 0.77813886]\n",
      "Layer 2 output: [0.34750771]\n",
      "Network output: [0.34750771]\n",
      "\n",
      "Gradients:\n",
      "Layer 1:\n",
      "  Bias gradients: [-0.00170086  0.00809399  0.0043312  -0.00330216]\n",
      "  Weight gradients shape: (4, 3)\n",
      "Layer 2:\n",
      "  Bias gradients: [-0.05694746]\n",
      "  Weight gradients shape: (1, 4)\n",
      "\n",
      "Processing example 4:\n",
      "Input: [0.15601864 0.15599452 0.05808361]\n",
      "Target output: [0.86617615]\n",
      "Layer 1 output: [0.16148972 0.57655867 0.62035382 0.55515885]\n",
      "Layer 2 output: [0.39113784]\n",
      "Network output: [0.39113784]\n",
      "\n",
      "Gradients:\n",
      "Layer 1:\n",
      "  Bias gradients: [-0.01427314  0.02304631  0.00812737 -0.00938392]\n",
      "  Weight gradients shape: (4, 3)\n",
      "Layer 2:\n",
      "  Bias gradients: [-0.11312991]\n",
      "  Weight gradients shape: (1, 4)\n",
      "\n",
      "Updating weights and biases:\n",
      "\n",
      "Layer 1:\n",
      "Weight update magnitude: 0.0005415081892356674\n",
      "Bias update magnitude: 0.0010820738489221748\n",
      "\n",
      "Layer 2:\n",
      "Weight update magnitude: 0.005197092783456625\n",
      "Bias update magnitude: 0.004754482364211166\n",
      "\n",
      "Processing mini-batch 3:\n",
      "\n",
      "Simulating update_mini_batch:\n",
      "Mini-batch size: 4\n",
      "Learning rate (eta): 0.1\n",
      "The shape of the biases: (4, 1)\n",
      "The shape of the biases: (1, 1)\n",
      "The shape of the weights: (4, 3)\n",
      "The shape of the weights: (1, 4)\n",
      "\n",
      "Processing example 1:\n",
      "Input: [0.59241457 0.04645041 0.60754485]\n",
      "Target output: [0.17052412]\n",
      "Layer 1 output: [0.11799078 0.54472016 0.37145401 0.60495161]\n",
      "Layer 2 output: [0.41230086]\n",
      "Network output: [0.41230086]\n",
      "\n",
      "Gradients:\n",
      "Layer 1:\n",
      "  Bias gradients: [ 0.00568386 -0.01207815 -0.00413378  0.00474484]\n",
      "  Weight gradients shape: (4, 3)\n",
      "Layer 2:\n",
      "  Bias gradients: [0.05858465]\n",
      "  Weight gradients shape: (1, 4)\n",
      "\n",
      "Processing example 2:\n",
      "Input: [0.60111501 0.70807258 0.02058449]\n",
      "Target output: [0.96990985]\n",
      "Layer 1 output: [0.0711101  0.6621052  0.47476287 0.55476834]\n",
      "Layer 2 output: [0.36726234]\n",
      "Network output: [0.36726234]\n",
      "\n",
      "Gradients:\n",
      "Layer 1:\n",
      "  Bias gradients: [-0.00862377  0.02604574  0.010554   -0.0117227 ]\n",
      "  Weight gradients shape: (4, 3)\n",
      "Layer 2:\n",
      "  Bias gradients: [-0.14004366]\n",
      "  Weight gradients shape: (1, 4)\n",
      "\n",
      "Processing example 3:\n",
      "Input: [0.30461377 0.09767211 0.68423303]\n",
      "Target output: [0.44015249]\n",
      "Layer 1 output: [0.11345952 0.59686671 0.49212097 0.67510247]\n",
      "Layer 2 output: [0.39777844]\n",
      "Network output: [0.39777844]\n",
      "\n",
      "Gradients:\n",
      "Layer 1:\n",
      "  Bias gradients: [-0.00095187  0.00203043  0.00076675 -0.00075454]\n",
      "  Weight gradients shape: (4, 3)\n",
      "Layer 2:\n",
      "  Bias gradients: [-0.01015074]\n",
      "  Weight gradients shape: (1, 4)\n",
      "\n",
      "Processing example 4:\n",
      "Input: [0.83244264 0.21233911 0.18182497]\n",
      "Target output: [0.18340451]\n",
      "Layer 1 output: [0.11684449 0.524444   0.32487548 0.48156528]\n",
      "Layer 2 output: [0.40940809]\n",
      "Network output: [0.40940809]\n",
      "\n",
      "Gradients:\n",
      "Layer 1:\n",
      "  Bias gradients: [ 0.00525706 -0.01132987 -0.00362228  0.00462354]\n",
      "  Weight gradients shape: (4, 3)\n",
      "Layer 2:\n",
      "  Bias gradients: [0.05464611]\n",
      "  Weight gradients shape: (1, 4)\n",
      "\n",
      "Updating weights and biases:\n",
      "\n",
      "Layer 1:\n",
      "Weight update magnitude: 0.0005326420126102191\n",
      "Bias update magnitude: 0.0001696092395504861\n",
      "\n",
      "Layer 2:\n",
      "Weight update magnitude: 0.0013707077261869245\n",
      "Bias update magnitude: 0.0009240909932115033\n",
      "\n",
      "Epoch 1 complete\n",
      "\n",
      "Epoch 2:\n",
      "\n",
      "Processing mini-batch 1:\n",
      "\n",
      "Simulating update_mini_batch:\n",
      "Mini-batch size: 4\n",
      "Learning rate (eta): 0.1\n",
      "The shape of the biases: (4, 1)\n",
      "The shape of the biases: (1, 1)\n",
      "The shape of the weights: (4, 3)\n",
      "The shape of the weights: (1, 4)\n",
      "\n",
      "Processing example 1:\n",
      "Input: [0.45606998 0.78517596 0.19967378]\n",
      "Target output: [0.51423444]\n",
      "Layer 1 output: [0.05902525 0.70724277 0.52757676 0.63418317]\n",
      "Layer 2 output: [0.35906197]\n",
      "Network output: [0.35906197]\n",
      "\n",
      "Gradients:\n",
      "Layer 1:\n",
      "  Bias gradients: [-0.00184897  0.00613962  0.0026828  -0.00281238]\n",
      "  Weight gradients shape: (4, 3)\n",
      "Layer 2:\n",
      "  Bias gradients: [-0.03571084]\n",
      "  Weight gradients shape: (1, 4)\n",
      "\n",
      "Processing example 2:\n",
      "Input: [0.37454012 0.95071431 0.73199394]\n",
      "Target output: [0.59865848]\n",
      "Layer 1 output: [0.03317439 0.78195263 0.52562694 0.77827337]\n",
      "Layer 2 output: [0.35067564]\n",
      "Network output: [0.35067564]\n",
      "\n",
      "Gradients:\n",
      "Layer 1:\n",
      "  Bias gradients: [-0.00168831  0.0079944   0.00424383 -0.00330777]\n",
      "  Weight gradients shape: (4, 3)\n",
      "Layer 2:\n",
      "  Bias gradients: [-0.05646625]\n",
      "  Weight gradients shape: (1, 4)\n",
      "\n",
      "Processing example 3:\n",
      "Input: [0.30461377 0.09767211 0.68423303]\n",
      "Target output: [0.44015249]\n",
      "Layer 1 output: [0.11344937 0.59686075 0.49210294 0.67511326]\n",
      "Layer 2 output: [0.39832248]\n",
      "Network output: [0.39832248]\n",
      "\n",
      "Gradients:\n",
      "Layer 1:\n",
      "  Bias gradients: [-0.00093995  0.002003    0.00075525 -0.00074643]\n",
      "  Weight gradients shape: (4, 3)\n",
      "Layer 2:\n",
      "  Bias gradients: [-0.01002505]\n",
      "  Weight gradients shape: (1, 4)\n",
      "\n",
      "Processing example 4:\n",
      "Input: [0.60111501 0.70807258 0.02058449]\n",
      "Target output: [0.96990985]\n",
      "Layer 1 output: [0.07111111 0.66201893 0.47470772 0.55482081]\n",
      "Layer 2 output: [0.36780981]\n",
      "Network output: [0.36780981]\n",
      "\n",
      "Gradients:\n",
      "Layer 1:\n",
      "  Bias gradients: [-0.00862092  0.02601165  0.01052298 -0.01173879]\n",
      "  Weight gradients shape: (4, 3)\n",
      "Layer 2:\n",
      "  Bias gradients: [-0.14000377]\n",
      "  Weight gradients shape: (1, 4)\n",
      "\n",
      "Updating weights and biases:\n",
      "\n",
      "Layer 1:\n",
      "Weight update magnitude: 0.0011889770798415899\n",
      "Bias update magnitude: 0.001281027031551297\n",
      "\n",
      "Layer 2:\n",
      "Weight update magnitude: 0.006406533020600928\n",
      "Bias update magnitude: 0.006055147706220877\n",
      "\n",
      "Processing mini-batch 2:\n",
      "\n",
      "Simulating update_mini_batch:\n",
      "Mini-batch size: 4\n",
      "Learning rate (eta): 0.1\n",
      "The shape of the biases: (4, 1)\n",
      "The shape of the biases: (1, 1)\n",
      "The shape of the weights: (4, 3)\n",
      "The shape of the weights: (1, 4)\n",
      "\n",
      "Processing example 1:\n",
      "Input: [0.25877998 0.66252228 0.31171108]\n",
      "Target output: [0.52006802]\n",
      "Layer 1 output: [0.06860157 0.70685708 0.59258655 0.67369326]\n",
      "Layer 2 output: [0.36285319]\n",
      "Network output: [0.36285319]\n",
      "\n",
      "Gradients:\n",
      "Layer 1:\n",
      "  Bias gradients: [-0.00216582  0.0062221   0.00261868 -0.00274254]\n",
      "  Weight gradients shape: (4, 3)\n",
      "Layer 2:\n",
      "  Bias gradients: [-0.03634661]\n",
      "  Weight gradients shape: (1, 4)\n",
      "\n",
      "Processing example 2:\n",
      "Input: [0.15601864 0.15599452 0.05808361]\n",
      "Target output: [0.86617615]\n",
      "Layer 1 output: [0.16160153 0.57597372 0.62010455 0.55542051]\n",
      "Layer 2 output: [0.39724696]\n",
      "Network output: [0.39724696]\n",
      "\n",
      "Gradients:\n",
      "Layer 1:\n",
      "  Bias gradients: [-0.01418703  0.02265504  0.00789345 -0.00951653]\n",
      "  Weight gradients shape: (4, 3)\n",
      "Layer 2:\n",
      "  Bias gradients: [-0.11228125]\n",
      "  Weight gradients shape: (1, 4)\n",
      "\n",
      "Processing example 3:\n",
      "Input: [0.83244264 0.21233911 0.18182497]\n",
      "Target output: [0.18340451]\n",
      "Layer 1 output: [0.11689197 0.52397574 0.32468277 0.48178355]\n",
      "Layer 2 output: [0.41270325]\n",
      "Network output: [0.41270325]\n",
      "\n",
      "Gradients:\n",
      "Layer 1:\n",
      "  Bias gradients: [ 0.00535037 -0.01145248 -0.00363659  0.00476278]\n",
      "  Weight gradients shape: (4, 3)\n",
      "Layer 2:\n",
      "  Bias gradients: [0.05557726]\n",
      "  Weight gradients shape: (1, 4)\n",
      "\n",
      "Processing example 4:\n",
      "Input: [0.12203823 0.49517691 0.03438852]\n",
      "Target output: [0.9093204]\n",
      "Layer 1 output: [0.1071366  0.66186524 0.6611959  0.60562125]\n",
      "Layer 2 output: [0.36964642]\n",
      "Network output: [0.36964642]\n",
      "\n",
      "Gradients:\n",
      "Layer 1:\n",
      "  Bias gradients: [-0.01121795  0.02325004  0.00840643 -0.01030901]\n",
      "  Weight gradients shape: (4, 3)\n",
      "Layer 2:\n",
      "  Bias gradients: [-0.12574833]\n",
      "  Weight gradients shape: (1, 4)\n",
      "\n",
      "Updating weights and biases:\n",
      "\n",
      "Layer 1:\n",
      "Weight update magnitude: 0.0005299899273493697\n",
      "Bias update magnitude: 0.0012987363290497278\n",
      "\n",
      "Layer 2:\n",
      "Weight update magnitude: 0.006354675854594281\n",
      "Bias update magnitude: 0.0054699732502903955\n",
      "\n",
      "Processing mini-batch 3:\n",
      "\n",
      "Simulating update_mini_batch:\n",
      "Mini-batch size: 4\n",
      "Learning rate (eta): 0.1\n",
      "The shape of the biases: (4, 1)\n",
      "The shape of the biases: (1, 1)\n",
      "The shape of the weights: (4, 3)\n",
      "The shape of the weights: (1, 4)\n",
      "\n",
      "Processing example 1:\n",
      "Input: [0.06505159 0.94888554 0.96563203]\n",
      "Target output: [0.80839735]\n",
      "Layer 1 output: [0.03097362 0.8167536  0.63557735 0.84637094]\n",
      "Layer 2 output: [0.34781961]\n",
      "Network output: [0.34781961]\n",
      "\n",
      "Gradients:\n",
      "Layer 1:\n",
      "  Bias gradients: [-0.00292659  0.01286206  0.00712699 -0.0047092 ]\n",
      "  Weight gradients shape: (4, 3)\n",
      "Layer 2:\n",
      "  Bias gradients: [-0.10447797]\n",
      "  Weight gradients shape: (1, 4)\n",
      "\n",
      "Processing example 2:\n",
      "Input: [0.61185289 0.13949386 0.29214465]\n",
      "Target output: [0.36636184]\n",
      "Layer 1 output: [0.12708367 0.53934681 0.39895999 0.53642138]\n",
      "Layer 2 output: [0.41374222]\n",
      "Network output: [0.41374222]\n",
      "\n",
      "Gradients:\n",
      "Layer 1:\n",
      "  Bias gradients: [ 0.00118984 -0.00234865 -0.00081163  0.00099068]\n",
      "  Weight gradients shape: (4, 3)\n",
      "Layer 2:\n",
      "  Bias gradients: [0.01149257]\n",
      "  Weight gradients shape: (1, 4)\n",
      "\n",
      "Processing example 3:\n",
      "Input: [0.59241457 0.04645041 0.60754485]\n",
      "Target output: [0.17052412]\n",
      "Layer 1 output: [0.11808817 0.54407213 0.37118454 0.60523667]\n",
      "Layer 2 output: [0.41854149]\n",
      "Network output: [0.41854149]\n",
      "\n",
      "Gradients:\n",
      "Layer 1:\n",
      "  Bias gradients: [ 0.00586652 -0.01231548 -0.00414916  0.00499907]\n",
      "  Weight gradients shape: (4, 3)\n",
      "Layer 2:\n",
      "  Bias gradients: [0.06035863]\n",
      "  Weight gradients shape: (1, 4)\n",
      "\n",
      "Processing example 4:\n",
      "Input: [0.30424224 0.52475643 0.43194502]\n",
      "Target output: [0.29122914]\n",
      "Layer 1 output: [0.0755555  0.68001875 0.55061941 0.67575209]\n",
      "Layer 2 output: [0.37550238]\n",
      "Network output: [0.37550238]\n",
      "\n",
      "Gradients:\n",
      "Layer 1:\n",
      "  Bias gradients: [ 0.00128822 -0.00353702 -0.00144015  0.00150102]\n",
      "  Weight gradients shape: (4, 3)\n",
      "Layer 2:\n",
      "  Bias gradients: [0.0197621]\n",
      "  Weight gradients shape: (1, 4)\n",
      "\n",
      "Updating weights and biases:\n",
      "\n",
      "Layer 1:\n",
      "Weight update magnitude: 0.0004187210987437101\n",
      "Bias update magnitude: 0.00020329296098277983\n",
      "\n",
      "Layer 2:\n",
      "Weight update magnitude: 0.0013667442926441307\n",
      "Bias update magnitude: 0.00032161699486281994\n",
      "\n",
      "Epoch 2 complete\n"
     ]
    }
   ],
   "source": [
    "# Create sample training data\n",
    "np.random.seed(42)  # for reproducibility\n",
    "training_data = [(np.random.rand(3, 1), np.random.rand(1, 1)) for _ in range(12)]\n",
    "for i in range(len(training_data)):\n",
    "    print(f\"The {i+1}th training example is: {training_data[i]}\")\n",
    "\n",
    "# Run the simulation\n",
    "simulate_sgd(training_data, epochs=2, mini_batch_size=4, eta=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3004d9fd-6175-4306-b3c9-9116d6f356bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
