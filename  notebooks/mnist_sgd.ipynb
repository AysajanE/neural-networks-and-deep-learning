{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec1c145e-90fc-4156-bd11-ad8c974af193",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "799c3c35-feba-433b-bfd8-3aff5491b1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    The sigmoid function.\n",
    "\n",
    "    Parameters:\n",
    "    z (numpy.ndarray): The input to the sigmoid function.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: The sigmoid of the input.\n",
    "    \"\"\"\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"\n",
    "    Derivative of the sigmoid function.\n",
    "\n",
    "    Parameters:\n",
    "    z (numpy.ndarray): The input to the sigmoid function.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: The derivative of the sigmoid function at z.\n",
    "    \"\"\"\n",
    "    return sigmoid(z) * (1 - sigmoid(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0589bef1-dfd4-4bd9-8629-0e27b790685f",
   "metadata": {},
   "source": [
    "## 3-layer NN\n",
    "![This is an example image](http://neuralnetworksanddeeplearning.com/images/tikz10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe57e65f-0036-4663-81fb-c2ee4f15ffa5",
   "metadata": {},
   "source": [
    "## Weights initilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ca779ec-ef79-4976-a20d-194906f223e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 4\n",
      "4 1\n",
      "[array([[-0.46651846, -1.12942651,  0.19766863],\n",
      "       [ 2.40491053,  0.33854765,  0.67934898],\n",
      "       [-1.54463351, -1.82830397, -0.04256603],\n",
      "       [ 0.13525649, -0.10129711,  0.34887896]]), array([[ 2.23689866, -0.44323522, -0.05001511,  1.58402228]])]\n"
     ]
    }
   ],
   "source": [
    "# Size contains the number of neurons in the respective layers\n",
    "sizes = [3, 4, 1]\n",
    "# Understand dimensions of weights\n",
    "for x, y in zip(sizes[:-1], sizes[1:]):\n",
    "    print(x, y)\n",
    "# Initialize weights using Gaussian distribution with mean 0 and standard deviation 1\n",
    "weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd0643a-df10-4fd8-8d9f-3669c4f40ad9",
   "metadata": {},
   "source": [
    "For the above network, the number of weights for each neuron in a layer is determined by the number of neurons in the previous layer. For instance, the second layer (i.e., hidden layer) in the above 3-layer NN has four neurons. For each neuron in this layer there are three inputs from the three neurons in the previous layer, which requires three weights. Therefore, the dimension of the array to store all the weights for this layer has shape of (4, 3), with the first dimension corresponds to the number of neurons in the current layer and second dimension corresponds to the total number of neurons in the previous layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d74278-2934-4be9-a0b0-09a287b61515",
   "metadata": {},
   "source": [
    "## Bias initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75829ff4-c53a-4ea2-9ef5-481825cd0799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "1\n",
      "[array([[ 1.5446769 ],\n",
      "       [ 0.04072502],\n",
      "       [ 1.05776775],\n",
      "       [-0.81686915]]), array([[1.07602797]])]\n"
     ]
    }
   ],
   "source": [
    "# Understand dimensions of bias\n",
    "for y in sizes[1:]:\n",
    "    print(y)\n",
    "# Initialize bias using Gaussian distribution with mean 0 and standard deviation 1    \n",
    "bias = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "print(bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d04318-e32f-448b-bd18-f24197f23b2b",
   "metadata": {},
   "source": [
    "For bias, firstly no bias term is needed for the input layer since the neurons in this layer do not have any outputs coming in. The number of bias terms for all the subsequent layes, including output layer, is equal to the number of neurons in each layer, which is captured in the sizes of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0878c023-70f7-409f-a2e6-772c259018c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self, sizes):\n",
    "        \"\"\"\n",
    "        Initialize the neural network with the given sizes.\n",
    "        \n",
    "        Parameters:\n",
    "        sizes (list): A list containing the number of neurons in each layer.\n",
    "                      For example, [784, 30, 10] would create a network with\n",
    "                      784 input neurons, one hidden layer with 30 neurons,\n",
    "                      and an output layer with 10 neurons.\n",
    "        \"\"\"\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        # Biases and weights are initialized with Gaussian distribution\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "\n",
    "    def feedforward(self, a):\n",
    "        \"\"\"\n",
    "        Return the output of the network if 'a' is input.\n",
    "        \n",
    "        Parameters:\n",
    "        a (numpy array): The input to the network.\n",
    "        \n",
    "        Returns:\n",
    "        numpy array: The output of the network.\n",
    "        \"\"\"\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a) + b)\n",
    "        return a\n",
    "\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n",
    "        \"\"\"\n",
    "        Train the neural network using mini-batch stochastic gradient descent.\n",
    "        \n",
    "        Parameters:\n",
    "        training_data (list): A list of tuples '(x, y)' representing the training inputs\n",
    "                              and the desired outputs.\n",
    "        epochs (int): The number of epochs to train for.\n",
    "        mini_batch_size (int): The size of the mini-batches to use when sampling.\n",
    "        eta (float): The learning rate.\n",
    "        test_data (list, optional): If provided, the network will be evaluated\n",
    "                                    against the test data after each epoch, and\n",
    "                                    partial progress will be printed out.\n",
    "        \"\"\"\n",
    "        if test_data: n_test = len(test_data)\n",
    "        n = len(training_data)\n",
    "        for j in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [training_data[k:k+mini_batch_size] for k in range(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            if test_data:\n",
    "                print(f\"Epoch {j}: {self.evaluate(test_data)} / {n_test}\")\n",
    "            else:\n",
    "                print(f\"Epoch {j} complete\")\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        \"\"\"\n",
    "        Update the network's weights and biases by applying gradient descent\n",
    "        using backpropagation to a single mini-batch.\n",
    "        \n",
    "        Parameters:\n",
    "        mini_batch (list): A list of tuples '(x, y)'.\n",
    "        eta (float): The learning rate.\n",
    "        \"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb + dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw + dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [w - (eta / len(mini_batch)) * nw for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b - (eta / len(mini_batch)) * nb for b, nb in zip(self.biases, nabla_b)]\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"\n",
    "        Return a tuple representing the gradient for the cost function.\n",
    "        \n",
    "        Parameters:\n",
    "        x (numpy array): The input to the network.\n",
    "        y (numpy array): The desired output.\n",
    "        \n",
    "        Returns:\n",
    "        tuple: (nabla_b, nabla_w), representing the gradients for the biases and weights.\n",
    "        \"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # Feedforward\n",
    "        activation = x\n",
    "        activations = [x]  # List to store all the activations, layer by layer\n",
    "        zs = []  # List to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation) + b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        # Backward pass\n",
    "        delta = self.cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        # Update the gradients for the previous layers\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "    def evaluate(self, test_data):\n",
    "        \"\"\"\n",
    "        Return the number of test inputs for which the neural network outputs\n",
    "        the correct result.\n",
    "        \n",
    "        Parameters:\n",
    "        test_data (list): A list of tuples '(x, y)' where 'x' is the input and 'y' is the desired output.\n",
    "        \n",
    "        Returns:\n",
    "        int: The number of test inputs for which the network is correct.\n",
    "        \"\"\"\n",
    "        test_results = [(np.argmax(self.feedforward(x)), y) for (x, y) in test_data]\n",
    "        return sum(int(x == y) for (x, y) in test_results)\n",
    "\n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        \"\"\"\n",
    "        Return the vector of partial derivatives ∂C/∂a for the output activations.\n",
    "        \n",
    "        Parameters:\n",
    "        output_activations (numpy array): The output of the network.\n",
    "        y (numpy array): The desired output.\n",
    "        \n",
    "        Returns:\n",
    "        numpy array: The derivative of the cost function.\n",
    "        \"\"\"\n",
    "        return (output_activations - y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d4af14-972b-441c-b20c-cf6ad5313462",
   "metadata": {},
   "source": [
    "## Create a Network object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ea457f3-eac3-4931-9196-92ae79db4fda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 4)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A neural network with three layers: one input layer with three neurons, one hidden layer with four neurons and \n",
    "# one output layer with one neuron\n",
    "net = Network([3, 4, 1])\n",
    "# Weights connecting the second layer and third layers of neurons\n",
    "net.weights[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28dbc1d-5af7-44be-945e-e6b8627c216c",
   "metadata": {},
   "source": [
    "## Understand `feedforward` function of `Network` object\n",
    "Let's walk through an example of the `feedforward` function using a neural network with sizes `[3, 4, 1]`. This means the network has:\n",
    "- 3 neurons in the input layer\n",
    "- 4 neurons in the hidden layer\n",
    "- 1 neuron in the output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacc50d2-6b44-4a8c-ac31-cab5715e764f",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "For the sake of this example, let's assume the following initial random weights and biases for the network:\n",
    "\n",
    "#### Weights\n",
    "- Between input layer and hidden layer (3 input neurons to 4 hidden neurons):\n",
    "  ```python\n",
    "  w1 = [[0.2, -0.5, 1.0],\n",
    "        [1.5, -1.0, 0.5],\n",
    "        [-1.5, 2.0, -1.0],\n",
    "        [0.5, -0.5, 1.5]]\n",
    "  ```\n",
    "- Between hidden layer and output layer (4 hidden neurons to 1 output neuron):\n",
    "  ```python\n",
    "  w2 = [[0.3, -0.8, 0.5, 1.0]]\n",
    "  ```\n",
    "\n",
    "#### Biases\n",
    "- For hidden layer (4 neurons):\n",
    "  ```python\n",
    "  b1 = [[0.1],\n",
    "        [0.2],\n",
    "        [0.3],\n",
    "        [0.4]]\n",
    "  ```\n",
    "- For output layer (1 neuron):\n",
    "  ```python\n",
    "  b2 = [[0.5]]\n",
    "  ```\n",
    "\n",
    "### Input\n",
    "Input layer of this NN has three neurons. Let's take a specific input training example for those three input neurons:\n",
    "```python\n",
    "input_vector = [[0.5],\n",
    "                [0.1],\n",
    "                [0.4]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1ed5b89e-0d38-4579-8abf-881f1628733a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<svg width=\"800\" height=\"400\" viewBox=\"0 0 800 400\" xmlns=\"http://www.w3.org/2000/svg\">\n",
       "  <svg viewBox=\"0 0 800 400\" xmlns=\"http://www.w3.org/2000/svg\">\n",
       "  <!-- Input Layer -->\n",
       "  <g id=\"input-layer\">\n",
       "    <circle cx=\"100\" cy=\"80\" r=\"20\" fill=\"lightblue\" stroke=\"black\"/>\n",
       "    <text x=\"100\" y=\"85\" text-anchor=\"middle\" font-size=\"12\">0.5</text>\n",
       "    <circle cx=\"100\" cy=\"160\" r=\"20\" fill=\"lightblue\" stroke=\"black\"/>\n",
       "    <text x=\"100\" y=\"165\" text-anchor=\"middle\" font-size=\"12\">0.1</text>\n",
       "    <circle cx=\"100\" cy=\"240\" r=\"20\" fill=\"lightblue\" stroke=\"black\"/>\n",
       "    <text x=\"100\" y=\"245\" text-anchor=\"middle\" font-size=\"12\">0.4</text>\n",
       "  </g>\n",
       "  \n",
       "  <!-- Hidden Layer -->\n",
       "  <g id=\"hidden-layer\">\n",
       "    <circle cx=\"300\" cy=\"60\" r=\"20\" fill=\"lightgreen\" stroke=\"black\"/>\n",
       "    <text x=\"300\" y=\"65\" text-anchor=\"middle\" font-size=\"12\">0.63</text>\n",
       "    <circle cx=\"300\" cy=\"140\" r=\"20\" fill=\"lightgreen\" stroke=\"black\"/>\n",
       "    <text x=\"300\" y=\"145\" text-anchor=\"middle\" font-size=\"12\">0.74</text>\n",
       "    <circle cx=\"300\" cy=\"220\" r=\"20\" fill=\"lightgreen\" stroke=\"black\"/>\n",
       "    <text x=\"300\" y=\"225\" text-anchor=\"middle\" font-size=\"12\">0.34</text>\n",
       "    <circle cx=\"300\" cy=\"300\" r=\"20\" fill=\"lightgreen\" stroke=\"black\"/>\n",
       "    <text x=\"300\" y=\"305\" text-anchor=\"middle\" font-size=\"12\">0.77</text>\n",
       "  </g>\n",
       "  \n",
       "  <!-- Output Layer -->\n",
       "  <g id=\"output-layer\">\n",
       "    <circle cx=\"500\" cy=\"160\" r=\"20\" fill=\"lightyellow\" stroke=\"black\"/>\n",
       "    <text x=\"500\" y=\"165\" text-anchor=\"middle\" font-size=\"12\">0.74</text>\n",
       "  </g>\n",
       "  \n",
       "  <!-- Connections -->\n",
       "  <g id=\"connections\" stroke=\"gray\" stroke-width=\"1\">\n",
       "    <!-- Input to Hidden -->\n",
       "    <line x1=\"120\" y1=\"80\" x2=\"280\" y2=\"60\"/>\n",
       "    <line x1=\"120\" y1=\"80\" x2=\"280\" y2=\"140\"/>\n",
       "    <line x1=\"120\" y1=\"80\" x2=\"280\" y2=\"220\"/>\n",
       "    <line x1=\"120\" y1=\"80\" x2=\"280\" y2=\"300\"/>\n",
       "    \n",
       "    <line x1=\"120\" y1=\"160\" x2=\"280\" y2=\"60\"/>\n",
       "    <line x1=\"120\" y1=\"160\" x2=\"280\" y2=\"140\"/>\n",
       "    <line x1=\"120\" y1=\"160\" x2=\"280\" y2=\"220\"/>\n",
       "    <line x1=\"120\" y1=\"160\" x2=\"280\" y2=\"300\"/>\n",
       "    \n",
       "    <line x1=\"120\" y1=\"240\" x2=\"280\" y2=\"60\"/>\n",
       "    <line x1=\"120\" y1=\"240\" x2=\"280\" y2=\"140\"/>\n",
       "    <line x1=\"120\" y1=\"240\" x2=\"280\" y2=\"220\"/>\n",
       "    <line x1=\"120\" y1=\"240\" x2=\"280\" y2=\"300\"/>\n",
       "    \n",
       "    <!-- Hidden to Output -->\n",
       "    <line x1=\"320\" y1=\"60\" x2=\"480\" y2=\"160\"/>\n",
       "    <line x1=\"320\" y1=\"140\" x2=\"480\" y2=\"160\"/>\n",
       "    <line x1=\"320\" y1=\"220\" x2=\"480\" y2=\"160\"/>\n",
       "    <line x1=\"320\" y1=\"300\" x2=\"480\" y2=\"160\"/>\n",
       "  </g>\n",
       "  \n",
       "  <!-- Weights -->\n",
       "  <g id=\"weights\" font-size=\"10\" fill=\"red\">\n",
       "    <!-- Input to Hidden -->\n",
       "    <text x=\"150\" y=\"70\">0.2</text>\n",
       "    <text x=\"150\" y=\"90\">1.5</text>\n",
       "    <text x=\"150\" y=\"110\">-1.5</text>\n",
       "    <text x=\"150\" y=\"130\">0.5</text>\n",
       "    \n",
       "    <text x=\"180\" y=\"100\">-0.5</text>\n",
       "    <text x=\"180\" y=\"150\">-1.0</text>\n",
       "    <text x=\"180\" y=\"180\">2.0</text>\n",
       "    <text x=\"180\" y=\"210\">-0.5</text>\n",
       "    \n",
       "    <text x=\"230\" y=\"100\">1.0</text>\n",
       "    <text x=\"230\" y=\"160\">0.5</text>\n",
       "    <text x=\"230\" y=\"220\">-1.0</text>\n",
       "    <text x=\"230\" y=\"280\">1.5</text>\n",
       "    \n",
       "    <!-- Hidden to Output -->\n",
       "    <text x=\"400\" y=\"100\">0.3</text>\n",
       "    <text x=\"400\" y=\"140\">-0.8</text>\n",
       "    <text x=\"400\" y=\"180\">0.5</text>\n",
       "    <text x=\"400\" y=\"220\">1.0</text>\n",
       "  </g>\n",
       "  \n",
       "  <!-- Labels -->\n",
       "  <text x=\"100\" y=\"20\" text-anchor=\"middle\" font-size=\"14\" font-weight=\"bold\">Input Layer</text>\n",
       "  <text x=\"300\" y=\"20\" text-anchor=\"middle\" font-size=\"14\" font-weight=\"bold\">Hidden Layer</text>\n",
       "  <text x=\"500\" y=\"20\" text-anchor=\"middle\" font-size=\"14\" font-weight=\"bold\">Output Layer</text>\n",
       "  <text x=\"600\" y=\"160\" font-size=\"14\">Output: 0.74</text>\n",
       "</svg>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<svg width=\"800\" height=\"400\" viewBox=\"0 0 800 400\" xmlns=\"http://www.w3.org/2000/svg\">\n",
    "  <svg viewBox=\"0 0 800 400\" xmlns=\"http://www.w3.org/2000/svg\">\n",
    "  <!-- Input Layer -->\n",
    "  <g id=\"input-layer\">\n",
    "    <circle cx=\"100\" cy=\"80\" r=\"20\" fill=\"lightblue\" stroke=\"black\"/>\n",
    "    <text x=\"100\" y=\"85\" text-anchor=\"middle\" font-size=\"12\">0.5</text>\n",
    "    <circle cx=\"100\" cy=\"160\" r=\"20\" fill=\"lightblue\" stroke=\"black\"/>\n",
    "    <text x=\"100\" y=\"165\" text-anchor=\"middle\" font-size=\"12\">0.1</text>\n",
    "    <circle cx=\"100\" cy=\"240\" r=\"20\" fill=\"lightblue\" stroke=\"black\"/>\n",
    "    <text x=\"100\" y=\"245\" text-anchor=\"middle\" font-size=\"12\">0.4</text>\n",
    "  </g>\n",
    "  \n",
    "  <!-- Hidden Layer -->\n",
    "  <g id=\"hidden-layer\">\n",
    "    <circle cx=\"300\" cy=\"60\" r=\"20\" fill=\"lightgreen\" stroke=\"black\"/>\n",
    "    <text x=\"300\" y=\"65\" text-anchor=\"middle\" font-size=\"12\">0.63</text>\n",
    "    <circle cx=\"300\" cy=\"140\" r=\"20\" fill=\"lightgreen\" stroke=\"black\"/>\n",
    "    <text x=\"300\" y=\"145\" text-anchor=\"middle\" font-size=\"12\">0.74</text>\n",
    "    <circle cx=\"300\" cy=\"220\" r=\"20\" fill=\"lightgreen\" stroke=\"black\"/>\n",
    "    <text x=\"300\" y=\"225\" text-anchor=\"middle\" font-size=\"12\">0.34</text>\n",
    "    <circle cx=\"300\" cy=\"300\" r=\"20\" fill=\"lightgreen\" stroke=\"black\"/>\n",
    "    <text x=\"300\" y=\"305\" text-anchor=\"middle\" font-size=\"12\">0.77</text>\n",
    "  </g>\n",
    "  \n",
    "  <!-- Output Layer -->\n",
    "  <g id=\"output-layer\">\n",
    "    <circle cx=\"500\" cy=\"160\" r=\"20\" fill=\"lightyellow\" stroke=\"black\"/>\n",
    "    <text x=\"500\" y=\"165\" text-anchor=\"middle\" font-size=\"12\">0.74</text>\n",
    "  </g>\n",
    "  \n",
    "  <!-- Connections -->\n",
    "  <g id=\"connections\" stroke=\"gray\" stroke-width=\"1\">\n",
    "    <!-- Input to Hidden -->\n",
    "    <line x1=\"120\" y1=\"80\" x2=\"280\" y2=\"60\"/>\n",
    "    <line x1=\"120\" y1=\"80\" x2=\"280\" y2=\"140\"/>\n",
    "    <line x1=\"120\" y1=\"80\" x2=\"280\" y2=\"220\"/>\n",
    "    <line x1=\"120\" y1=\"80\" x2=\"280\" y2=\"300\"/>\n",
    "    \n",
    "    <line x1=\"120\" y1=\"160\" x2=\"280\" y2=\"60\"/>\n",
    "    <line x1=\"120\" y1=\"160\" x2=\"280\" y2=\"140\"/>\n",
    "    <line x1=\"120\" y1=\"160\" x2=\"280\" y2=\"220\"/>\n",
    "    <line x1=\"120\" y1=\"160\" x2=\"280\" y2=\"300\"/>\n",
    "    \n",
    "    <line x1=\"120\" y1=\"240\" x2=\"280\" y2=\"60\"/>\n",
    "    <line x1=\"120\" y1=\"240\" x2=\"280\" y2=\"140\"/>\n",
    "    <line x1=\"120\" y1=\"240\" x2=\"280\" y2=\"220\"/>\n",
    "    <line x1=\"120\" y1=\"240\" x2=\"280\" y2=\"300\"/>\n",
    "    \n",
    "    <!-- Hidden to Output -->\n",
    "    <line x1=\"320\" y1=\"60\" x2=\"480\" y2=\"160\"/>\n",
    "    <line x1=\"320\" y1=\"140\" x2=\"480\" y2=\"160\"/>\n",
    "    <line x1=\"320\" y1=\"220\" x2=\"480\" y2=\"160\"/>\n",
    "    <line x1=\"320\" y1=\"300\" x2=\"480\" y2=\"160\"/>\n",
    "  </g>\n",
    "  \n",
    "  <!-- Weights -->\n",
    "  <g id=\"weights\" font-size=\"10\" fill=\"red\">\n",
    "    <!-- Input to Hidden -->\n",
    "    <text x=\"150\" y=\"70\">0.2</text>\n",
    "    <text x=\"150\" y=\"90\">1.5</text>\n",
    "    <text x=\"150\" y=\"110\">-1.5</text>\n",
    "    <text x=\"150\" y=\"130\">0.5</text>\n",
    "    \n",
    "    <text x=\"180\" y=\"100\">-0.5</text>\n",
    "    <text x=\"180\" y=\"150\">-1.0</text>\n",
    "    <text x=\"180\" y=\"180\">2.0</text>\n",
    "    <text x=\"180\" y=\"210\">-0.5</text>\n",
    "    \n",
    "    <text x=\"230\" y=\"100\">1.0</text>\n",
    "    <text x=\"230\" y=\"160\">0.5</text>\n",
    "    <text x=\"230\" y=\"220\">-1.0</text>\n",
    "    <text x=\"230\" y=\"280\">1.5</text>\n",
    "    \n",
    "    <!-- Hidden to Output -->\n",
    "    <text x=\"400\" y=\"100\">0.3</text>\n",
    "    <text x=\"400\" y=\"140\">-0.8</text>\n",
    "    <text x=\"400\" y=\"180\">0.5</text>\n",
    "    <text x=\"400\" y=\"220\">1.0</text>\n",
    "  </g>\n",
    "  \n",
    "  <!-- Labels -->\n",
    "  <text x=\"100\" y=\"20\" text-anchor=\"middle\" font-size=\"14\" font-weight=\"bold\">Input Layer</text>\n",
    "  <text x=\"300\" y=\"20\" text-anchor=\"middle\" font-size=\"14\" font-weight=\"bold\">Hidden Layer</text>\n",
    "  <text x=\"500\" y=\"20\" text-anchor=\"middle\" font-size=\"14\" font-weight=\"bold\">Output Layer</text>\n",
    "  <text x=\"600\" y=\"160\" font-size=\"14\">Output: 0.74</text>\n",
    "</svg>\n",
    "</svg>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452e52ab-79e4-4d91-97d4-de47e6583488",
   "metadata": {},
   "source": [
    "### Feedforward Calculation\n",
    "\n",
    "Let's break down the `feedforward` function step by step:\n",
    "\n",
    "1. **Initialization**: Set the input vector `a` as the initial activation.\n",
    "\n",
    "    ```python\n",
    "    a = input_vector\n",
    "    ```\n",
    "\n",
    "2. **First Layer Calculation (Input to Hidden)**:\n",
    "   - Compute the weighted input $z_1$ for the hidden layer:\n",
    "     \\begin{equation}\n",
    "     z_1 = w_1 \\cdot a + b_1\n",
    "     \\end{equation}\n",
    "     Substituting the values:\n",
    "     $$\n",
    "     z_1 = \\begin{bmatrix}\n",
    "     0.2 & -0.5 & 1.0 \\\\\n",
    "     1.5 & -1.0 & 0.5 \\\\\n",
    "     -1.5 & 2.0 & -1.0 \\\\\n",
    "     0.5 & -0.5 & 1.5 \n",
    "     \\end{bmatrix} \\cdot \\begin{bmatrix}\n",
    "     0.5 \\\\\n",
    "     0.1 \\\\\n",
    "     0.4 \n",
    "     \\end{bmatrix} + \\begin{bmatrix}\n",
    "     0.1 \\\\\n",
    "     0.2 \\\\\n",
    "     0.3 \\\\\n",
    "     0.4 \n",
    "     \\end{bmatrix}\n",
    "     $$\n",
    "     \n",
    "     Calculate the dot product:\n",
    "     \n",
    "     $$\n",
    "     w_1 \\cdot a = \\begin{bmatrix}\n",
    "     0.2 \\cdot 0.5 + -0.5 \\cdot 0.1 + 1.0 \\cdot 0.4 \\\\\n",
    "     1.5 \\cdot 0.5 + -1.0 \\cdot 0.1 + 0.5 \\cdot 0.4 \\\\\n",
    "     -1.5 \\cdot 0.5 + 2.0 \\cdot 0.1 + -1.0 \\cdot 0.4 \\\\\n",
    "     0.5 \\cdot 0.5 + -0.5 \\cdot 0.1 + 1.5 \\cdot 0.4\n",
    "     \\end{bmatrix} = \\begin{bmatrix}\n",
    "     0.1 + -0.05 + 0.4 \\\\\n",
    "     0.75 + -0.1 + 0.2 \\\\\n",
    "     -0.75 + 0.2 + -0.4 \\\\\n",
    "     0.25 + -0.05 + 0.6\n",
    "     \\end{bmatrix} = \\begin{bmatrix}\n",
    "     0.45 \\\\\n",
    "     0.85 \\\\\n",
    "     -0.95 \\\\\n",
    "     0.8 \n",
    "     \\end{bmatrix}\n",
    "     $$\n",
    "     Adding the bias:\n",
    "     $$\n",
    "     z_1 = \\begin{bmatrix}\n",
    "     0.45 \\\\\n",
    "     0.85 \\\\\n",
    "     -0.95 \\\\\n",
    "     0.8 \n",
    "     \\end{bmatrix} + \\begin{bmatrix}\n",
    "     0.1 \\\\\n",
    "     0.2 \\\\\n",
    "     0.3 \\\\\n",
    "     0.4 \n",
    "     \\end{bmatrix} = \\begin{bmatrix}\n",
    "     0.55 \\\\\n",
    "     1.05 \\\\\n",
    "     -0.65 \\\\\n",
    "     1.2 \n",
    "     \\end{bmatrix}\n",
    "     $$\n",
    "\n",
    "   - Apply the sigmoid activation function to get the activation $a_1$ of the hidden layer:\n",
    "     $$\n",
    "     a_1 = \\sigma(z_1) = \\sigma\\left(\\begin{bmatrix}\n",
    "     0.55 \\\\\n",
    "     1.05 \\\\\n",
    "     -0.65 \\\\\n",
    "     1.2 \n",
    "     \\end{bmatrix}\\right) = \\begin{bmatrix}\n",
    "     \\sigma(0.55) \\\\\n",
    "     \\sigma(1.05) \\\\\n",
    "     \\sigma(-0.65) \\\\\n",
    "     \\sigma(1.2)\n",
    "     \\end{bmatrix} = \\begin{bmatrix}\n",
    "     0.63413559 \\\\\n",
    "     0.7407749 \\\\\n",
    "     0.34298954 \\\\\n",
    "     0.76852478\n",
    "     \\end{bmatrix}\n",
    "     $$\n",
    "\n",
    "3. **Second Layer Calculation (Hidden to Output)**:\n",
    "   - Compute the weighted input $z_2$ for the output layer:\n",
    "     $$\n",
    "     z_2 = w_2 \\cdot a_1 + b_2\n",
    "     $$\n",
    "     \n",
    "     Substituting the values:\n",
    "     \n",
    "     $$\n",
    "     z_2 = \\begin{bmatrix}\n",
    "     0.3 & -0.8 & 0.5 & 1.0\n",
    "     \\end{bmatrix} \\cdot \\begin{bmatrix}\n",
    "     0.63413559 \\\\\n",
    "     0.7407749 \\\\\n",
    "     0.34298954 \\\\\n",
    "     0.76852478\n",
    "     \\end{bmatrix} + \\begin{bmatrix}\n",
    "     0.5\n",
    "     \\end{bmatrix}\n",
    "     $$\n",
    "     Calculate the dot product:\n",
    "     $$\n",
    "     w_2 \\cdot a_1 = 0.3 \\cdot 0.63413559 + -0.8 \\cdot 0.7407749 + 0.5 \\cdot 0.34298954 + 1.0 \\cdot 0.76852478 = 0.19024068 + -0.59261992 + 0.17149477 + 0.76852478 = 0.53764031\n",
    "     $$\n",
    "     Adding the bias:\n",
    "     $$\n",
    "     z_2 = 0.53764031 + 0.5 = 1.03764031\n",
    "     $$\n",
    "\n",
    "   - Apply the sigmoid activation function to get the activation `a2` of the output layer:\n",
    "     $$\n",
    "     a_2 = \\sigma(z_2) = \\sigma(1.03764031) = 0.73841914\n",
    "     $$\n",
    "\n",
    "### Final Output\n",
    "So, given the input vector `[[0.5], [0.1], [0.4]]`, the network produces an output of `0.73841914`.\n",
    "\n",
    "### Summary\n",
    "- **Input Layer**: Receives the input `[[0.5], [0.1], [0.4]]`.\n",
    "- **Hidden Layer**: \n",
    "  - Computes $z_1$ using weights and biases.\n",
    "  - Applies the sigmoid function to $z_1$ to get $a_1$.\n",
    "- **Output Layer**:\n",
    "  - Computes $z_2$ using weights and biases.\n",
    "  - Applies the sigmoid function to $z_2$ to get $a_2$.\n",
    "\n",
    "This step-by-step example demonstrates how the `feedforward` function processes an input through the layers of the network to produce the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2dca4acd-2bb2-465d-9c8b-a5d00fb877f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network Feedforward Result:\n",
      "Input: [[0.5 0.1 0.4]]\n",
      "Output: 0.73839445\n",
      "\n",
      "Comparison:\n",
      "Manual calculation result: 0.73841914\n",
      "Network feedforward result: 0.73839445\n",
      "Difference: 0.00002469\n"
     ]
    }
   ],
   "source": [
    "# Create a network with the same structure as our previous example\n",
    "net = Network([3, 4, 1])\n",
    "\n",
    "# Set the weights and biases to match our previous example\n",
    "net.weights = [\n",
    "    np.array([[0.2, -0.5, 1.0],\n",
    "              [1.5, -1.0, 0.5],\n",
    "              [-1.5, 2.0, -1.0],\n",
    "              [0.5, -0.5, 1.5]]),\n",
    "    np.array([[0.3, -0.8, 0.5, 1.0]])\n",
    "]\n",
    "\n",
    "net.biases = [\n",
    "    np.array([[0.1], [0.2], [0.3], [0.4]]),\n",
    "    np.array([[0.5]])\n",
    "]\n",
    "\n",
    "# Input vector\n",
    "input_vector = np.array([[0.5], [0.1], [0.4]])\n",
    "\n",
    "# Use the feedforward method\n",
    "output = net.feedforward(input_vector)\n",
    "\n",
    "print(\"Network Feedforward Result:\")\n",
    "print(f\"Input: {input_vector.T}\")\n",
    "print(f\"Output: {output[0][0]:.8f}\")\n",
    "\n",
    "# Compare with our previous manual calculation\n",
    "print(\"\\nComparison:\")\n",
    "print(f\"Manual calculation result: 0.73841914\")\n",
    "print(f\"Network feedforward result: {output[0][0]:.8f}\")\n",
    "print(f\"Difference: {abs(0.73841914 - output[0][0]):.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "87a8e492-6c75-4ab6-8c6c-72703917e14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network Feedforward Result:\n",
      "Input: [[0.5 0.1 0.4]]\n",
      "Output: 0.50672075\n"
     ]
    }
   ],
   "source": [
    "# Create a network with the same structure as our previous example\n",
    "# The weights and biases are initialized by the Network object.\n",
    "net = Network([3, 4, 1])\n",
    "\n",
    "# Input vector\n",
    "input_vector = np.array([[0.5], [0.1], [0.4]])\n",
    "\n",
    "# Use the feedforward method\n",
    "output = net.feedforward(input_vector)\n",
    "\n",
    "print(\"Network Feedforward Result:\")\n",
    "print(f\"Input: {input_vector.T}\")\n",
    "print(f\"Output: {output[0][0]:.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b86247-6295-4770-bf7b-2acdc0a88d49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
