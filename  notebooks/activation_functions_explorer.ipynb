{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd7711cd-2821-42f1-8620-a7d474649137",
   "metadata": {},
   "source": [
    "# Neural Networks and Deep Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3743fcf-e3f3-4ba8-aa8f-ffc1352cb953",
   "metadata": {},
   "source": [
    "## Chapter 1: Using neural nets to recognize handwritten digits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb8530f-f1d1-4a6d-8bc7-e9562f5813cb",
   "metadata": {},
   "source": [
    "### Explanation of the Sigmoid Function:\n",
    "\n",
    "The sigmoid function, also known as the logistic function, is a mathematical function that takes any real-valued number and maps it to a value between 0 and 1. It's defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma (x)=\\dfrac{1}{1+e^{-x}}\n",
    "\\end{equation}\n",
    "\n",
    "Where:\n",
    "- $\\sigma (x)$ is the sigmoid function\n",
    "- $e$ is the base of the natural logarithm (Euler's number, approximately 2.71828)\n",
    "- $x$ is the input value\n",
    "\n",
    "The sigmoid function is often used in machine learning, particularly in neural networks, because of its useful properties:\n",
    "- It's smooth and continuous\n",
    "- It's bounded between 0 and 1\n",
    "- It has a characteristic S-shaped curve\n",
    "- Its derivative is easy to calculate, which is useful for training neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0063e8fd-6304-43de-a729-d53f791f36ad",
   "metadata": {},
   "source": [
    "## Python Function for Sigmoid:\n",
    "\n",
    "Let's write a Python function to calculate the sigmoid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe1eeac-6aa7-4536-a9fe-fccd6f5921ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f63513a-3024-4851-82f8-ccdbd880c880",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Calculates the sigmoid of x.\n",
    "    \n",
    "    Args:\n",
    "    z (float): Input value\n",
    "    \n",
    "    Returns:\n",
    "    float: Sigmoid of z\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f0ffd1-46be-473d-adbd-0af9ed8d6cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sigmoid():\n",
    "    \"\"\"\n",
    "    Creates and displays a plot of the sigmoid function.\n",
    "    \"\"\"\n",
    "    x = np.linspace(-10, 10, 1000)\n",
    "    y = [sigmoid(i) for i in x]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(x, y)\n",
    "    plt.title('Sigmoid Function')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('σ(x)')\n",
    "    plt.grid(True)\n",
    "    plt.axhline(y=0, color='r', linestyle='--')\n",
    "    plt.axhline(y=1, color='r', linestyle='--')\n",
    "    plt.axvline(x=0, color='g', linestyle='--')\n",
    "    plt.text(0, 0.5, '0.5', ha='left', va='center')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a5b9e2-857d-4ef0-85de-b59f81d7b845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate some values\n",
    "test_values = [-5, -2, -1, 0, 1, 2, 5]\n",
    "\n",
    "print(\"x\\t\\tSigmoid(x)\")\n",
    "print(\"-----------------------\")\n",
    "for x in test_values:\n",
    "    print(f\"{x}\\t\\t{sigmoid(x):.6f}\")\n",
    "\n",
    "# Plot the sigmoid function\n",
    "plot_sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d543731-c01e-4286-9e4e-dce5563b48ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_cost_one_var(X, y, w, b):\n",
    "    \"\"\"\n",
    "    Compute the cost function for linear regression with one variable.\n",
    "    \n",
    "    Args:\n",
    "    X (ndarray): Shape (m,) array of input feature values, where m is the number of examples.\n",
    "    y (ndarray): Shape (m,) array of target values.\n",
    "    w (float): Weight parameter.\n",
    "    b (float): Bias parameter.\n",
    "    \n",
    "    Returns:\n",
    "    float: The cost value.\n",
    "    \n",
    "    The cost function is defined as:\n",
    "    J(w,b) = (1/2m) * Σ(h(x) - y)^2\n",
    "    where h(x) = wx + b, and m is the number of training examples.\n",
    "    \"\"\"\n",
    "    m = len(X)\n",
    "    h = w * X + b  # Compute predictions\n",
    "    cost = (1 / (2 * m)) * np.sum((h - y) ** 2)  # Compute mean squared error\n",
    "    return cost\n",
    "\n",
    "def linear_cost_two_var(X, y, w, b):\n",
    "    \"\"\"\n",
    "    Compute the cost function for linear regression with two variables.\n",
    "    \n",
    "    Args:\n",
    "    X (ndarray): Shape (m, 2) array of input feature values, where m is the number of examples.\n",
    "    y (ndarray): Shape (m,) array of target values.\n",
    "    w (ndarray): Shape (2,) array of weight parameters.\n",
    "    b (float): Bias parameter.\n",
    "    \n",
    "    Returns:\n",
    "    float: The cost value.\n",
    "    \n",
    "    The cost function is defined as:\n",
    "    J(w,b) = (1/2m) * Σ(h(x) - y)^2\n",
    "    where h(x) = w1*x1 + w2*x2 + b, and m is the number of training examples.\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    h = np.dot(X, w) + b  # Compute predictions\n",
    "    cost = (1 / (2 * m)) * np.sum((h - y) ** 2)  # Compute mean squared error\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55933e56-d511-45b1-bd21-3f2f05c797e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "# One variable example\n",
    "X_one = np.array([1, 2, 3, 4, 5])\n",
    "y_one = np.array([2, 4, 6, 8, 10])\n",
    "w_one = 2\n",
    "b_one = 0\n",
    "cost_one = linear_cost_one_var(X_one, y_one, w_one, b_one)\n",
    "print(f\"Cost for one variable: {cost_one}\")\n",
    "\n",
    "# Two variable example\n",
    "X_two = np.array([[1, 1], [2, 2], [3, 3], [4, 4], [5, 5]])\n",
    "y_two = np.array([3, 6, 9, 12, 15])\n",
    "w_two = np.array([1, 1])\n",
    "b_two = 1\n",
    "cost_two = linear_cost_two_var(X_two, y_two, w_two, b_two)\n",
    "print(f\"Cost for two variables: {cost_two}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bf0332-33a3-4023-87b5-cb7ae940ce99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_cost_one_var(X, y, w, b):\n",
    "    \"\"\"\n",
    "    Compute the squared error cost function for logistic regression with one variable.\n",
    "    \n",
    "    Args:\n",
    "    X (ndarray): Shape (m,) array of input feature values, where m is the number of examples.\n",
    "    y (ndarray): Shape (m,) array of target values (0 or 1).\n",
    "    w (float): Weight parameter.\n",
    "    b (float): Bias parameter.\n",
    "    \n",
    "    Returns:\n",
    "    float: The cost value.\n",
    "    \n",
    "    The cost function is defined as:\n",
    "    J(w,b) = (1/2m) * Σ(h(x) - y)^2\n",
    "    where h(x) = sigmoid(wx + b), and m is the number of training examples.\n",
    "    \"\"\"\n",
    "    m = len(X)\n",
    "    z = w * X + b\n",
    "    h = sigmoid(z)\n",
    "    \n",
    "    # Compute the cost using squared error\n",
    "    cost = (1 / (2 * m)) * np.sum((h - y) ** 2)\n",
    "    \n",
    "    return cost\n",
    "\n",
    "def logistic_cost_two_var(X, y, w, b):\n",
    "    \"\"\"\n",
    "    Compute the squared error cost function for logistic regression with two variables.\n",
    "    \n",
    "    Args:\n",
    "    X (ndarray): Shape (m, 2) array of input feature values, where m is the number of examples.\n",
    "    y (ndarray): Shape (m,) array of target values (0 or 1).\n",
    "    w (ndarray): Shape (2,) array of weight parameters.\n",
    "    b (float): Bias parameter.\n",
    "    \n",
    "    Returns:\n",
    "    float: The cost value.\n",
    "    \n",
    "    The cost function is defined as:\n",
    "    J(w,b) = (1/2m) * Σ(h(x) - y)^2\n",
    "    where h(x) = sigmoid(w1*x1 + w2*x2 + b), and m is the number of training examples.\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    z = np.dot(X, w) + b\n",
    "    h = sigmoid(z)\n",
    "    \n",
    "    # Compute the cost using squared error\n",
    "    cost = (1 / (2 * m)) * np.sum((h - y) ** 2)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37669e2e-da5d-4007-8f4d-86305fed43bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "\n",
    "# One variable example\n",
    "X_one = np.array([1, 2, 3, 4, 5])\n",
    "y_one = np.array([0, 0, 1, 1, 1])\n",
    "w_one = 0.5\n",
    "b_one = -1\n",
    "cost_one = logistic_cost_one_var(X_one, y_one, w_one, b_one)\n",
    "print(f\"Logistic Regression Cost (Squared Error) for one variable: {cost_one}\")\n",
    "\n",
    "# Two variable example\n",
    "X_two = np.array([[1, 1], [2, 2], [3, 3], [4, 4], [5, 5]])\n",
    "y_two = np.array([0, 0, 0, 1, 1])\n",
    "w_two = np.array([0.5, 0.5])\n",
    "b_two = -3\n",
    "cost_two = logistic_cost_two_var(X_two, y_two, w_two, b_two)\n",
    "print(f\"Logistic Regression Cost (Squared Error) for two variables: {cost_two}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38e374a-7395-4891-a003-a7bbf32d3e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "X_linear = np.array([1.0, 1.7, 2.0, 2.5, 3.0, 3.2])\n",
    "y_linear = np.array([250, 300, 480, 430, 630, 730])\n",
    "\n",
    "X_logistic = np.array([0, 1, 2, 3, 4, 5])\n",
    "y_logistic = np.array([0, 0, 0, 1, 1, 1])\n",
    "\n",
    "# Set b and w range\n",
    "b_linear = 100\n",
    "b_logistic = 0\n",
    "w_range_linear = np.linspace(-100, 500, 1000)\n",
    "w_range_logistic = np.linspace(-5, 5, 1000)\n",
    "\n",
    "# Calculate costs for different w values\n",
    "linear_costs = [linear_cost_one_var(X_linear, y_linear, w, b_linear) for w in w_range_linear]\n",
    "logistic_costs = [logistic_cost_one_var(X_logistic, y_logistic, w, b_logistic) for w in w_range_logistic]\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Linear Regression Cost Plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(w_range, linear_costs)\n",
    "plt.title('Linear Regression Cost Function')\n",
    "plt.xlabel('Weight (w)')\n",
    "plt.ylabel('Cost')\n",
    "plt.grid(True)\n",
    "\n",
    "# Logistic Regression Cost Plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(w_range, logistic_costs)\n",
    "plt.title('Logistic Regression Cost Function')\n",
    "plt.xlabel('Weight (w)')\n",
    "plt.ylabel('Cost')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print minimum cost and corresponding w for each regression\n",
    "linear_min_cost = min(linear_costs)\n",
    "linear_best_w = w_range[np.argmin(linear_costs)]\n",
    "print(f\"Linear Regression - Minimum Cost: {linear_min_cost:.2f}, Best w: {linear_best_w:.2f}\")\n",
    "\n",
    "logistic_min_cost = min(logistic_costs)\n",
    "logistic_best_w = w_range[np.argmin(logistic_costs)]\n",
    "print(f\"Logistic Regression - Minimum Cost: {logistic_min_cost:.2f}, Best w: {logistic_best_w:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf36ce0-1361-48c7-be47-3767b94daf3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
